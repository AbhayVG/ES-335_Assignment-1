{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  -------- ES335- MACHINE LEARNING  ASSIGNMENT - 1 --------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                        ------- TASK-3 ------- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q.1 Demonstrate how to use Zero-Shot Learning and Few-Shot Learning to classify human activities based on the featurized accelerometer data. Qualitatively demonstrate the performance of Few-Shot Learning with Zero-Shot Learning. Which method performs better? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def load_data(folder_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for class_folder in os.listdir(folder_path):\n",
    "        class_path = os.path.join(folder_path, class_folder)\n",
    "        for file in os.listdir(class_path):\n",
    "            file_path = os.path.join(class_path, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            data.append(df.values)\n",
    "            labels.append(class_folder)\n",
    "    return data, labels\n",
    "\n",
    "train_data, train_labels = load_data(r'D:\\Users\\abhay\\Downloads\\ES335-Assignment-1-1\\Combined\\Train')\n",
    "test_data, test_labels = load_data(r'D:\\Users\\abhay\\Downloads\\ES335-Assignment-1-1\\Combined\\Test')\n",
    "\n",
    "train_data, train_labels = shuffle(train_data, train_labels, random_state=42)\n",
    "test_data, test_labels = shuffle(test_data, test_labels, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Activity for sample 1 using Zero-Shot Learning: content='Walking' response_metadata={'token_usage': {'completion_tokens': 2, 'prompt_tokens': 78009, 'total_tokens': 78011, 'completion_time': 0.0080612, 'prompt_time': 18.485065554, 'queue_time': 20.129402244, 'total_time': 18.493126754}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_5c5d1b5cfb', 'finish_reason': 'stop', 'logprobs': None} id='run-9669fe6d-2604-461b-a350-f6e5f254189c-0' usage_metadata={'input_tokens': 78009, 'output_tokens': 2, 'total_tokens': 78011}\n",
      "\n",
      "Actual Activity for sample 1: STANDING\n",
      "\n",
      "Predicted Activity for sample 2 using Zero-Shot Learning: content='Walking' response_metadata={'token_usage': {'completion_tokens': 2, 'prompt_tokens': 62965, 'total_tokens': 62967, 'completion_time': 0.008304642, 'prompt_time': 15.253448142, 'queue_time': 0.015097390999999405, 'total_time': 15.261752784}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_9260b4bb2e', 'finish_reason': 'stop', 'logprobs': None} id='run-217fe017-ad86-49c3-8de2-ab375be61980-0' usage_metadata={'input_tokens': 62965, 'output_tokens': 2, 'total_tokens': 62967}\n",
      "\n",
      "Actual Activity for sample 2: WALKING_UPSTAIRS\n",
      "\n",
      "Predicted Activity for sample 3 using Zero-Shot Learning: content='' response_metadata={'token_usage': {'completion_tokens': 0, 'prompt_tokens': 0, 'total_tokens': 0, 'completion_time': 0.0, 'prompt_time': 0.0, 'queue_time': None, 'total_time': 0.0}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-5906b82c-b295-4939-bef7-250f3c51900f-0' role=''\n",
      "\n",
      "Actual Activity for sample 3: WALKING_UPSTAIRS\n",
      "\n",
      "Predicted Activity for sample 4 using Zero-Shot Learning: content='Walking' response_metadata={'token_usage': {'completion_tokens': 2, 'prompt_tokens': 74252, 'total_tokens': 74254, 'completion_time': 0.008111302, 'prompt_time': 18.151607931, 'queue_time': 9.718937805, 'total_time': 18.159719233}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_9260b4bb2e', 'finish_reason': 'stop', 'logprobs': None} id='run-503ded32-41e2-47df-9011-f68f0c64d1d4-0' usage_metadata={'input_tokens': 74252, 'output_tokens': 2, 'total_tokens': 74254}\n",
      "\n",
      "Actual Activity for sample 4: SITTING\n",
      "\n",
      "Predicted Activity for sample 5 using Zero-Shot Learning: content='Walking' response_metadata={'token_usage': {'completion_tokens': 2, 'prompt_tokens': 55119, 'total_tokens': 55121, 'completion_time': 0.008081292, 'prompt_time': 13.293446704, 'queue_time': 10.560503983, 'total_time': 13.301527996}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_9260b4bb2e', 'finish_reason': 'stop', 'logprobs': None} id='run-dcfe7a0d-58f0-4610-9894-68ec1ac980e4-0' usage_metadata={'input_tokens': 55119, 'output_tokens': 2, 'total_tokens': 55121}\n",
      "\n",
      "Actual Activity for sample 5: WALKING_DOWNSTAIRS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq.chat_models import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "groq_api_key = os.getenv(\"API_FINAL_KEY\")\n",
    "\n",
    "# Initialize the chat model with specific settings\n",
    "chat = ChatGroq(temperature=0.1, groq_api_key=groq_api_key, model_name=\"llama-3.1-70b-versatile\")\n",
    "\n",
    "# Define the system prompt for Zero-Shot Learning\n",
    "system = \"Demonstrate how to use Zero-Shot Learning to classify human activities based on the featurized accelerometer data. Give just predicted activity only, nothing else.\"\n",
    "\n",
    "# Define the human prompt template\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "# Create the chain that will process each input\n",
    "chain = prompt | chat\n",
    "\n",
    "# Loop through the test data to make predictions and print the results\n",
    "for i in range(5):  \n",
    "    data_summary = f\"Sample data: {test_data[i].flatten().tolist()}...\"  \n",
    "    result = chain.invoke({\"text\": data_summary})\n",
    "    \n",
    "    # Output the predicted activity based on Zero-Shot Learning\n",
    "    print(f\"Predicted Activity for sample {i+1} using Zero-Shot Learning: {result}\\n\")\n",
    "    print(f\"Actual Activity for sample {i+1}: {test_labels[i]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Activity for sample 1 using Few-Shot Learning: content='WALKING' response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 651, 'total_tokens': 655, 'completion_time': 0.016285594, 'prompt_time': 0.194076009, 'queue_time': 6.882396721, 'total_time': 0.210361603}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_9260b4bb2e', 'finish_reason': 'stop', 'logprobs': None} id='run-3e83acca-683f-4593-803f-238ab9ec2156-0' usage_metadata={'input_tokens': 651, 'output_tokens': 4, 'total_tokens': 655}\n",
      "\n",
      "Actual Activity for sample 1: STANDING\n",
      "\n",
      "Predicted Activity for sample 2 using Few-Shot Learning: content='WALKING_UPSTAIRS' response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 654, 'total_tokens': 662, 'completion_time': 0.032, 'prompt_time': 0.186586116, 'queue_time': 0.005273799999999995, 'total_time': 0.218586116}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_5c5d1b5cfb', 'finish_reason': 'stop', 'logprobs': None} id='run-db4c6980-1954-4182-9dfb-7dc921741b07-0' usage_metadata={'input_tokens': 654, 'output_tokens': 8, 'total_tokens': 662}\n",
      "\n",
      "Actual Activity for sample 2: WALKING_UPSTAIRS\n",
      "\n",
      "Predicted Activity for sample 3 using Few-Shot Learning: content='WALKING_UPSTAIRS' response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 659, 'total_tokens': 667, 'completion_time': 0.032, 'prompt_time': 0.202375297, 'queue_time': 0.0061872479999999785, 'total_time': 0.234375297}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_5c5d1b5cfb', 'finish_reason': 'stop', 'logprobs': None} id='run-cba8e50f-ddd3-49d4-aecc-f177c90028bd-0' usage_metadata={'input_tokens': 659, 'output_tokens': 8, 'total_tokens': 667}\n",
      "\n",
      "Actual Activity for sample 3: WALKING_UPSTAIRS\n",
      "\n",
      "Predicted Activity for sample 4 using Few-Shot Learning: content='WALKING' response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 657, 'total_tokens': 661, 'completion_time': 0.016047316, 'prompt_time': 0.165064997, 'queue_time': 0.006460533000000018, 'total_time': 0.181112313}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_b3ae7e594e', 'finish_reason': 'stop', 'logprobs': None} id='run-dc88e73b-1022-45a4-8d21-b61723442950-0' usage_metadata={'input_tokens': 657, 'output_tokens': 4, 'total_tokens': 661}\n",
      "\n",
      "Actual Activity for sample 4: SITTING\n",
      "\n",
      "Predicted Activity for sample 5 using Few-Shot Learning: content='WALKING' response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 653, 'total_tokens': 657, 'completion_time': 0.016, 'prompt_time': 0.172899225, 'queue_time': 0.005533901000000008, 'total_time': 0.188899225}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_b6828be2c9', 'finish_reason': 'stop', 'logprobs': None} id='run-06631d3a-57f7-4da2-afdc-bc30d1714768-0' usage_metadata={'input_tokens': 653, 'output_tokens': 4, 'total_tokens': 657}\n",
      "\n",
      "Actual Activity for sample 5: WALKING_DOWNSTAIRS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq.chat_models import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "groq_api_key = os.getenv(\"API_FINAL_KEY\")\n",
    "\n",
    "\n",
    "# Initialize the chat model with specific settings\n",
    "chat = ChatGroq(temperature=0, groq_api_key=groq_api_key, model_name=\"llama-3.1-70b-versatile\")\n",
    "\n",
    "# Define the system prompt for Few-Shot Learning\n",
    "system = \"Demonstrate how to use Few-Shot Learning to classify human activities based on the featurized accelerometer data. Give just predicted activity only, nothing else.\"\n",
    "\n",
    "# Define the human prompt template\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "# Create the chain that will process each input\n",
    "chain = prompt | chat\n",
    "\n",
    "# Prepare examples for Few-Shot Learning\n",
    "examples = \"\"\n",
    "for i in range(3):  # Use first 3 samples as examples\n",
    "    data_summary = f\"Data: {train_data[i].flatten().tolist()[:20]} => Activity: {train_labels[i]}\"\n",
    "    examples += data_summary + \"\\n\"\n",
    "\n",
    "# Loop through the test data to make predictions and print the results\n",
    "for i in range(5):  \n",
    "    new_data_summary = f\"New data: {test_data[i].flatten().tolist()[:20]}\"\n",
    "    few_shot_prompt = f\"Here are some examples:\\n{examples}\\nNow classify the following:\\n{new_data_summary}\"\n",
    "    \n",
    "    # Get the result from Few-Shot Learning model\n",
    "    result = chain.invoke({\"text\": few_shot_prompt})\n",
    "    \n",
    "    # Output the predicted activity based on Few-Shot Learning\n",
    "    print(f\"Predicted Activity for sample {i+1} using Few-Shot Learning: {result}\\n\")\n",
    "    print(f\"Actual Activity for sample {i+1}: {test_labels[i]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Zero Shot and Few Shot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1 Comparison:\n",
      "Zero-Shot Predicted Activity: Walking\n",
      "Few-Shot Predicted Activity: WALKING\n",
      "Actual Activity: STANDING\n",
      "\n",
      "Sample 2 Comparison:\n",
      "Zero-Shot Predicted Activity: Walking\n",
      "Few-Shot Predicted Activity: WALKING_UPSTAIRS\n",
      "Actual Activity: WALKING_UPSTAIRS\n",
      "\n",
      "Sample 3 Comparison:\n",
      "Zero-Shot Predicted Activity: Walking\n",
      "Few-Shot Predicted Activity: WALKING_UPSTAIRS\n",
      "Actual Activity: WALKING_UPSTAIRS\n",
      "\n",
      "Sample 4 Comparison:\n",
      "Zero-Shot Predicted Activity: Walking\n",
      "Few-Shot Predicted Activity: WALKING\n",
      "Actual Activity: SITTING\n",
      "\n",
      "Sample 5 Comparison:\n",
      "Zero-Shot Predicted Activity: Walking\n",
      "Few-Shot Predicted Activity: WALKING\n",
      "Actual Activity: WALKING_DOWNSTAIRS\n",
      "\n",
      "Zero-Shot Learning Accuracy: 0.00%\n",
      "Few-Shot Learning Accuracy: 40.00%\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq.chat_models import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "groq_api_key = os.getenv(\"API_FINAL_KEY\")\n",
    "\n",
    "\n",
    "# Initialize the chat model with specific settings\n",
    "chat = ChatGroq(temperature=0.1, groq_api_key= groq_api_key, model_name=\"llama-3.1-70b-versatile\")\n",
    "\n",
    "# Define the system prompts\n",
    "zero_shot_system = \"Demonstrate how to use Zero-Shot Learning to classify human activities based on the featurized accelerometer data. Give just predicted activity only, nothing else.\"\n",
    "few_shot_system = \"Demonstrate how to use Few-Shot Learning to classify human activities based on the featurized accelerometer data. Give just predicted activity only, nothing else.\"\n",
    "\n",
    "# Define the human prompt template\n",
    "human = \"{text}\"\n",
    "\n",
    "# Create the chains for Zero-Shot and Few-Shot Learning\n",
    "zero_shot_prompt = ChatPromptTemplate.from_messages([(\"system\", zero_shot_system), (\"human\", human)])\n",
    "few_shot_prompt = ChatPromptTemplate.from_messages([(\"system\", few_shot_system), (\"human\", human)])\n",
    "\n",
    "zero_shot_chain = zero_shot_prompt | chat\n",
    "few_shot_chain = few_shot_prompt | chat\n",
    "\n",
    "# Prepare examples for Few-Shot Learning\n",
    "examples = \"\"\n",
    "for i in range(3):  # Use first 3 samples as examples\n",
    "    data_summary = f\"Data: {train_data[i].flatten().tolist()[:20]} => Activity: {train_labels[i]}\"\n",
    "    examples += data_summary + \"\\n\"\n",
    "\n",
    "# Initialize lists to store predictions\n",
    "zero_shot_predictions = []\n",
    "few_shot_predictions = []\n",
    "\n",
    "# Loop through the test data to make predictions and compare results\n",
    "for i in range(5):  \n",
    "    # Zero-Shot Learning\n",
    "    zero_shot_data_summary = f\"Sample data: {test_data[i].flatten().tolist()}...\"  \n",
    "    zero_shot_result = zero_shot_chain.invoke({\"text\": zero_shot_data_summary}).content.strip()\n",
    "    zero_shot_predictions.append(zero_shot_result)\n",
    "\n",
    "    # Few-Shot Learning\n",
    "    new_data_summary = f\"New data: {test_data[i].flatten().tolist()[:20]}\"\n",
    "    few_shot_prompt_text = f\"Here are some examples:\\n{examples}\\nNow classify the following:\\n{new_data_summary}\"\n",
    "    few_shot_result = few_shot_chain.invoke({\"text\": few_shot_prompt_text}).content.strip()\n",
    "    few_shot_predictions.append(few_shot_result)\n",
    "\n",
    "    # Output the comparison for the current sample\n",
    "    print(f\"Sample {i+1} Comparison:\")\n",
    "    print(f\"Zero-Shot Predicted Activity: {zero_shot_result}\")\n",
    "    print(f\"Few-Shot Predicted Activity: {few_shot_result}\")\n",
    "    print(f\"Actual Activity: {test_labels[i]}\\n\")\n",
    "\n",
    "# Calculate accuracy for Zero-Shot Learning\n",
    "zero_shot_accuracy = accuracy_score(test_labels[:5], zero_shot_predictions)\n",
    "print(f\"Zero-Shot Learning Accuracy: {zero_shot_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Calculate accuracy for Few-Shot Learning\n",
    "few_shot_accuracy = accuracy_score(test_labels[:5], few_shot_predictions)\n",
    "print(f\"Few-Shot Learning Accuracy: {few_shot_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q.2 Quantitatively compare the accuracy of Few-Shot Learning with Decision Trees (You may use a subset of the test set if you encounter rate-limiting issues). Which method performs better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of the test data\n",
    "subset_size = 50  # Adjust if necessary\n",
    "test_data_subset = test_data[:subset_size]\n",
    "test_labels_subset = test_labels[:subset_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a few-shot learning example using the first 3 training samples\n",
    "examples = \"\"\n",
    "for i in range(3):\n",
    "    data_summary = f\"Data: {train_data[i].flatten().tolist()[:20]} => Activity: {train_labels[i]}\"\n",
    "    examples += data_summary + \"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Activity for sample 1 using Few-Shot Learning: content='WALKING' response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 651, 'total_tokens': 655, 'completion_time': 0.01600399, 'prompt_time': 0.162329604, 'queue_time': 0.143937122, 'total_time': 0.178333594}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_5c5d1b5cfb', 'finish_reason': 'stop', 'logprobs': None} id='run-e1b19835-b2ef-48af-a506-c21b58d2bc84-0' usage_metadata={'input_tokens': 651, 'output_tokens': 4, 'total_tokens': 655}\n",
      "\n",
      "Actual Activity for sample 1: STANDING\n",
      "\n",
      "Predicted Activity for sample 2 using Few-Shot Learning: content='WALKING_UPSTAIRS' response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 654, 'total_tokens': 662, 'completion_time': 0.032, 'prompt_time': 0.1744633, 'queue_time': 3.125263157, 'total_time': 0.2064633}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_5c5d1b5cfb', 'finish_reason': 'stop', 'logprobs': None} id='run-db00fbb5-73df-4e51-bfa1-71d9e0b73edb-0' usage_metadata={'input_tokens': 654, 'output_tokens': 8, 'total_tokens': 662}\n",
      "\n",
      "Actual Activity for sample 2: WALKING_UPSTAIRS\n",
      "\n",
      "Predicted Activity for sample 3 using Few-Shot Learning: content='WALKING_UPSTAIRS' response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 659, 'total_tokens': 667, 'completion_time': 0.032, 'prompt_time': 0.17852229, 'queue_time': 0.005119706000000002, 'total_time': 0.21052229}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_b6828be2c9', 'finish_reason': 'stop', 'logprobs': None} id='run-478c11ff-9d36-47c4-b299-b2dcf503c9b6-0' usage_metadata={'input_tokens': 659, 'output_tokens': 8, 'total_tokens': 667}\n",
      "\n",
      "Actual Activity for sample 3: WALKING_UPSTAIRS\n",
      "\n",
      "Predicted Activity for sample 4 using Few-Shot Learning: content='WALKING' response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 657, 'total_tokens': 661, 'completion_time': 0.016035593, 'prompt_time': 0.223285511, 'queue_time': 0.005354042000000003, 'total_time': 0.239321104}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_9260b4bb2e', 'finish_reason': 'stop', 'logprobs': None} id='run-7d594f3c-ff2e-478e-8bf8-77e7b0312e24-0' usage_metadata={'input_tokens': 657, 'output_tokens': 4, 'total_tokens': 661}\n",
      "\n",
      "Actual Activity for sample 4: SITTING\n",
      "\n",
      "Predicted Activity for sample 5 using Few-Shot Learning: content='WALKING' response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 653, 'total_tokens': 657, 'completion_time': 0.016, 'prompt_time': 0.178830998, 'queue_time': 0.120169832, 'total_time': 0.194830998}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_b3ae7e594e', 'finish_reason': 'stop', 'logprobs': None} id='run-f3e8a46d-90e7-40a2-932e-be6e4af37690-0' usage_metadata={'input_tokens': 653, 'output_tokens': 4, 'total_tokens': 657}\n",
      "\n",
      "Actual Activity for sample 5: WALKING_DOWNSTAIRS\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [5, 0]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 42\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActual Activity for sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_labels[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Calculate accuracy for Few-Shot Learning\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m few_shot_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfew_shot_predictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFew-Shot Learning Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfew_shot_accuracy\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\abhay\\anaconda3\\envs\\assignment1\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\abhay\\anaconda3\\envs\\assignment1\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:231\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    229\u001b[0m xp, _, device \u001b[38;5;241m=\u001b[39m get_namespace_and_device(y_true, y_pred, sample_weight)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\abhay\\anaconda3\\envs\\assignment1\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:103\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    102\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n\u001b[1;32m--> 103\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    105\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\abhay\\anaconda3\\envs\\assignment1\\Lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [5, 0]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "groq_api_key = os.getenv(\"API_FINAL_KEY\")\n",
    "\n",
    "\n",
    "# Initialize the chat model with specific settings\n",
    "chat = ChatGroq(temperature=0, groq_api_key=groq_api_key, model_name=\"llama-3.1-70b-versatile\")\n",
    "\n",
    "# Define the system prompt for Few-Shot Learning\n",
    "system = \"Demonstrate how to use Few-Shot Learning to classify human activities based on the featurized accelerometer data. Give just predicted activity only, nothing else.\"\n",
    "\n",
    "# Define the human prompt template\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "# Create the chain that will process each input\n",
    "chain = prompt | chat\n",
    "\n",
    "# Prepare examples for Few-Shot Learning\n",
    "examples = \"\"\n",
    "for i in range(3):  # Use first 3 samples as examples\n",
    "    data_summary = f\"Data: {train_data[i].flatten().tolist()[:20]} => Activity: {train_labels[i]}\"\n",
    "    examples += data_summary + \"\\n\"\n",
    "\n",
    "# Loop through the test data to make predictions and print the results\n",
    "for i in range(5):  \n",
    "    new_data_summary = f\"New data: {test_data[i].flatten().tolist()[:20]}\"\n",
    "    few_shot_prompt = f\"Here are some examples:\\n{examples}\\nNow classify the following:\\n{new_data_summary}\"\n",
    "\n",
    "    # Get the result from Few-Shot Learning model\n",
    "    result = chain.invoke({\"text\": few_shot_prompt})\n",
    "    \n",
    "    # Output the predicted activity based on Few-Shot Learning\n",
    "    print(f\"Predicted Activity for sample {i+1} using Few-Shot Learning: {result}\\n\")\n",
    "    print(f\"Actual Activity for sample {i+1}: {test_labels[i]}\\n\")\n",
    "\n",
    "\n",
    "# Calculate accuracy for Few-Shot Learning\n",
    "few_shot_accuracy = accuracy_score(test_labels[:5], few_shot_predictions)\n",
    "print(f\"Few-Shot Learning Accuracy: {few_shot_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_flat: (126, 18240)\n",
      "Shape of X_test_flat: (50, 18240)\n",
      "Decision Tree Accuracy: 56.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def flatten_data(data, max_length=None):\n",
    "    flattened_data = []\n",
    "    if max_length is None:\n",
    "        max_length = max(len(np.array(sample).flatten()) for sample in data)\n",
    "    for sample in data:\n",
    "        flattened_sample = np.array(sample).flatten()\n",
    "        if len(flattened_sample) < max_length:\n",
    "            # Pad the flattened sample to the max length with zeros\n",
    "            flattened_sample = np.pad(flattened_sample, (0, max_length - len(flattened_sample)), mode='constant')\n",
    "        flattened_data.append(flattened_sample)\n",
    "    return np.array(flattened_data), max_length\n",
    "\n",
    "# Flatten and ensure consistent shapes with the same max_length\n",
    "X_train_flat, max_length = flatten_data(train_data)\n",
    "X_test_flat, _ = flatten_data(test_data_subset, max_length=max_length)\n",
    "\n",
    "# Print shapes to debug\n",
    "print(f\"Shape of X_train_flat: {X_train_flat.shape}\")\n",
    "print(f\"Shape of X_test_flat: {X_test_flat.shape}\")\n",
    "\n",
    "# Verify that the number of features match\n",
    "assert X_train_flat.shape[1] == X_test_flat.shape[1], \\\n",
    "    f\"Feature mismatch: X_train has {X_train_flat.shape[1]} features, but X_test has {X_test_flat.shape[1]} features.\"\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels_subset)\n",
    "\n",
    "# Split training data for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_flat, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Decision Tree model\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the subset of the test data\n",
    "y_pred = clf.predict(X_test_flat)\n",
    "\n",
    "# Calculate Decision Tree accuracy\n",
    "decision_tree_accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "print(f\"Decision Tree Accuracy: {decision_tree_accuracy:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Trees perform better than Few-Shot Learning.\n"
     ]
    }
   ],
   "source": [
    "if few_shot_accuracy > decision_tree_accuracy:\n",
    "    print(\"Few-Shot Learning performs better than Decision Trees.\")\n",
    "elif few_shot_accuracy < decision_tree_accuracy:\n",
    "    print(\"Decision Trees perform better than Few-Shot Learning.\")\n",
    "else:\n",
    "    print(\"Few-Shot Learning and Decision Trees have the same accuracy.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q.3  What are the limitations of Zero-Shot Learning and Few-Shot Learning in the context of classifying human activities based on featurized accelerometer data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Zero-Shot Learning (ZSL) and Few-Shot Learning (FSL) are both machine learning approaches that aim to classify data with limited or no labeled examples. However, in the context of classifying human activities based on featurized accelerometer data, both ZSL and FSL have several limitations. One major limitation of ZSL is that it relies heavily on the semantic relationships between classes, which can be challenging to define and capture in the case of human activities. For instance, the semantic difference between \"walking\" and \"jogging\" may not be well-represented in the feature space, leading to poor performance. Additionally, ZSL assumes that the feature space is well-represented by the seen classes, which may not be the case when dealing with diverse human activities. FSL, on the other hand, requires a small number of labeled examples for each new class, which can still be challenging to obtain, especially in cases where data collection is expensive or time-consuming. Furthermore, FSL models can suffer from overfitting to the few available examples, leading to poor generalization performance. Another limitation of both ZSL and FSL is that they often rely on hand-crafted features, which may not capture the underlying patterns and relationships in the accelerometer data. Moreover, both approaches can be sensitive to the choice of hyperparameters, feature extraction methods, and the quality of the available data, which can significantly impact their performance. Finally, both ZSL and FSL may not be able to handle the variability and complexity of human activities, such as the differences in movement patterns, speeds, and styles, which can lead to poor classification accuracy.', response_metadata={'token_usage': {'completion_tokens': 335, 'prompt_tokens': 77, 'total_tokens': 412, 'completion_time': 1.34, 'prompt_time': 0.018767537, 'queue_time': 0.005146292, 'total_time': 1.358767537}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_5c5d1b5cfb', 'finish_reason': 'stop', 'logprobs': None}, id='run-119bd9a5-ce5c-4809-b5c5-2979ab9d51c1-0', usage_metadata={'input_tokens': 77, 'output_tokens': 335, 'total_tokens': 412})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq.chat_models import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "groq_api_key = os.getenv('API_FINAL_KEY')\n",
    "\n",
    "chat = ChatGroq(temperature=0, groq_api_key=groq_api_key, model_name=\"llama-3.1-70b-versatile\")\n",
    "\n",
    "system = \"You are a helpful assistant. Provide detailed explanations in a paragraph\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | chat\n",
    "chain.invoke({\"text\": \"What are the limitations of Zero-Shot Learning and Few-Shot Learning in the context of classifying human activities based on featurized accelerometer data?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. Zero-Shot Learning (ZSL) and Few-Shot Learning (FSL) are both machine learning approaches that aim to classify data with limited or no labeled examples. However, in the context of classifying human activities based on featurized accelerometer data, both ZSL and FSL have several limitations. One major limitation of ZSL is that it relies heavily on the semantic relationships between classes, which can be challenging to define and may not always be accurate. For example, in activity recognition, the semantic relationship between \"walking\" and \"running\" may not be well-defined, leading to poor performance. Additionally, ZSL assumes that the test data is drawn from the same distribution as the training data, which may not be the case in real-world scenarios where new activities or variations of existing activities may be encountered. Few-Shot Learning, on the other hand, requires a small number of labeled examples for each new class, which can still be challenging to obtain, especially in cases where data collection is time-consuming or expensive. Furthermore, FSL models can suffer from overfitting to the few available examples, leading to poor generalization performance. Another limitation of both ZSL and FSL is that they often rely on pre-trained models or feature extractors, which may not be optimized for the specific activity recognition task at hand. Moreover, the featurized accelerometer data may not capture the nuances of human activities, leading to poor performance. Finally, both ZSL and FSL may not be able to handle the variability and complexity of human activities, such as the differences in movement patterns, speeds, and intensities, which can lead to poor classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q.4 What does the model classify when given input from an entirely new activity that it hasn't seen before? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"When given input from an entirely new activity that it hasn't seen before, a Large Language Model (LLM) like myself will attempt to classify it based on patterns and relationships learned from the vast amount of text data I was trained on. Since I haven't seen this specific activity before, I won't have any direct knowledge or explicit classification rules to apply. However, I will use various techniques to make an educated guess. I'll analyze the input text for contextual clues, such as keywords, phrases, and sentence structures, to identify potential connections to known concepts or categories. I might also rely on my understanding of linguistic patterns, such as verb tenses, noun phrases, and semantic roles, to infer the activity's characteristics. Additionally, I may use analogies or metaphors to relate the new activity to something I've seen before, even if it's not an exact match. Ultimately, my classification will be a probabilistic prediction, and I may provide multiple possible interpretations or ask for more context to refine my understanding. While I may not always be accurate, my ability to generalize and adapt to new information is a key aspect of my language understanding capabilities.\", response_metadata={'token_usage': {'completion_tokens': 233, 'prompt_tokens': 70, 'total_tokens': 303, 'completion_time': 0.932, 'prompt_time': 0.020939157, 'queue_time': 1.82782169, 'total_time': 0.952939157}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_b3ae7e594e', 'finish_reason': 'stop', 'logprobs': None}, id='run-7e240b01-061f-45ba-8618-0ec50a3fb140-0', usage_metadata={'input_tokens': 70, 'output_tokens': 233, 'total_tokens': 303})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq.chat_models import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "groq_api_key = os.getenv('API_FINAL_KEY')\n",
    "\n",
    "chat = ChatGroq(temperature=0, groq_api_key=groq_api_key, model_name=\"llama-3.1-70b-versatile\")\n",
    "\n",
    "system = \"You are a helpful assistant. Provide detailed explanations in a paragraph\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | chat\n",
    "chain.invoke({\"text\": \"What does the llm model classify when given input from an entirely new activity that it hasn't seen before? \"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. When given input from an entirely new activity that it hasn\\'t seen before, the LLM (Large Language Model) model will attempt to classify it based on patterns and relationships learned from its vast training data. Since the model has not been explicitly trained on this new activity, it will rely on its ability to generalize and make inferences. The model will analyze the input text and try to identify similarities with known activities or concepts it has learned about during training. This process is called \"zero-shot learning\" or \"out-of-distribution generalization.\" The model may use various techniques, such as semantic role labeling, entity recognition, or part-of-speech tagging, to understand the context and meaning of the input text. If the model is unable to find a clear match with a known activity, it may classify the input as \"unknown\" or \"unseen,\" or it may generate a response that indicates it is uncertain or lacks sufficient information to provide a confident classification. However, some advanced LLMs may also use techniques like \"few-shot learning\" or \"meta-learning\" to adapt quickly to new activities and learn from a few examples, even if they have not seen them before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q.5 Test the model with random data (ensuring the data has the same dimensions and range as the previous input) and report the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1 Comparison:\n",
      "Zero-Shot Predicted Activity: Zero-Shot Learning for Human Activity Classification\n",
      "=====================================================\n",
      "\n",
      "Zero-shot learning is a type of machine learning where a model is trained to recognize objects or activities that it has not seen before. In this case, we will use zero-shot learning to classify human activities based on featurized accelerometer data.\n",
      "\n",
      "Dataset\n",
      "--------\n",
      "\n",
      "The dataset consists of featurized accelerometer data, which is a sequence of numbers representing the acceleration of the device in three dimensions (x, y, z). The dataset is labeled with the corresponding human activity (e.g., walking, running, sitting, etc.).\n",
      "\n",
      "Model\n",
      "------\n",
      "\n",
      "We will use a transformer-based model, specifically the BERT (Bidirectional Encoder Representations from Transformers) model, which is pre-trained on a large corpus of text data. We will fine-tune the model on our dataset to adapt it to the task of human activity classification.\n",
      "\n",
      "Code\n",
      "-----\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "from transformers import BertTokenizer, BertModel\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Load the dataset\n",
      "data = ...  # load the dataset\n",
      "\n",
      "# Split the dataset into training and testing sets\n",
      "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
      "\n",
      "# Create a custom dataset class for our data\n",
      "class HumanActivityDataset(torch.utils.data.Dataset):\n",
      "    def __init__(self, data, tokenizer):\n",
      "        self.data = data\n",
      "        self.tokenizer = tokenizer\n",
      "\n",
      "    def __getitem__(self, idx):\n",
      "        sequence = self.data[idx]\n",
      "        labels = torch.tensor(sequence['label'])\n",
      "\n",
      "        encoding = self.tokenizer.encode_plus(\n",
      "            sequence['sequence'],\n",
      "            add_special_tokens=True,\n",
      "            max_length=512,\n",
      "            return_attention_mask=True,\n",
      "            return_tensors='pt',\n",
      "            padding='max_length',\n",
      "            truncation=True,\n",
      "        )\n",
      "\n",
      "        return {\n",
      "            'input_ids': encoding['input_ids'].flatten(),\n",
      "            'attention_mask': encoding['attention_mask'].flatten(),\n",
      "            'labels': labels\n",
      "        }\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)\n",
      "\n",
      "# Create a dataset instance\n",
      "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      "dataset = HumanActivityDataset(train_data, tokenizer)\n",
      "\n",
      "# Create a data loader\n",
      "batch_size = 32\n",
      "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
      "\n",
      "# Create a model instance\n",
      "class HumanActivityModel(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(HumanActivityModel, self).__init__()\n",
      "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
      "        self.dropout = nn.Dropout(0.1)\n",
      "        self.classifier = nn.Linear(self.bert.config.hidden_size, 8)  # 8 classes\n",
      "\n",
      "    def forward(self, input_ids, attention_mask):\n",
      "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
      "        pooled_output = outputs.pooler_output\n",
      "        pooled_output = self.dropout(pooled_output)\n",
      "        outputs = self.classifier(pooled_output)\n",
      "        return outputs\n",
      "\n",
      "model = HumanActivityModel()\n",
      "\n",
      "# Train the model\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "model.to(device)\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
      "\n",
      "for epoch in range(5):\n",
      "    model.train()\n",
      "    total_loss = 0\n",
      "    for batch in data_loader:\n",
      "        input_ids = batch['input_ids'].to(device)\n",
      "        attention_mask = batch['attention_mask'].to(device)\n",
      "        labels = batch['labels'].to(device)\n",
      "\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        outputs = model(input_ids, attention_mask)\n",
      "        loss = criterion(outputs, labels)\n",
      "\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        total_loss += loss.item()\n",
      "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(data_loader)}')\n",
      "\n",
      "    model.eval()\n",
      "    with torch.no_grad():\n",
      "        total_correct = 0\n",
      "        for batch in data_loader:\n",
      "            input_ids = batch['input_ids'].to(device)\n",
      "            attention_mask = batch['attention_mask'].to(device)\n",
      "            labels = batch['labels'].to(device)\n",
      "\n",
      "            outputs = model(input_ids, attention_mask)\n",
      "            _, predicted = torch.max(outputs, dim=1)\n",
      "            total_correct += (predicted == labels).sum().item()\n",
      "\n",
      "        accuracy = total_correct / len(train_data)\n",
      "        print(f'Epoch {epoch+1}, Accuracy: {accuracy:.4f}')\n",
      "\n",
      "# Evaluate the model on the test set\n",
      "test_dataset = HumanActivityDataset(test_data, tokenizer)\n",
      "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
      "\n",
      "model.eval()\n",
      "with torch.no_grad():\n",
      "    total_correct = 0\n",
      "    for batch in test_data_loader:\n",
      "        input_ids = batch['input_ids'].to(device)\n",
      "        attention_mask = batch['attention_mask'].to(device)\n",
      "        labels = batch['labels'].to(device)\n",
      "\n",
      "        outputs = model(input_ids, attention_mask)\n",
      "        _, predicted = torch.max(outputs, dim=1)\n",
      "        total_correct += (predicted == labels).sum().item()\n",
      "\n",
      "    accuracy = total_correct / len(test_data)\n",
      "    print(f'Test Accuracy: {accuracy:.4f}')\n",
      "```\n",
      "\n",
      "This code assumes that the dataset is stored in a list of dictionaries, where each dictionary contains the sequence of accelerometer data and the corresponding label. The code creates a custom dataset class and a data loader to feed the data to the model. The model is a transformer-based model that uses the BERT architecture. The model is trained on the training set and evaluated on the test set. The accuracy of the model is printed at each epoch and after the final evaluation on the test set.\n",
      "\n",
      "Note that this is just an example code and may need to be modified to fit the specific requirements of your dataset and task.\n",
      "Few-Shot Predicted Activity: To classify the new data, we will use a few-shot learning approach. Since we have only three examples of different activities, we will use a simple k-Nearest Neighbors (k-NN) algorithm.\n",
      "\n",
      "Here is a Python implementation of the k-NN algorithm:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "# Define the training data\n",
      "train_data = [\n",
      "    np.array([0.43285953675395783, 0.2743450980543033, 0.6870501079486016, 0.3998785751741558, 0.22948025718768883, 0.8529979549798196, 0.8782577095331902, 0.3539210421900467, 0.3198987259222472, 0.02917777646929809, 0.47068594145052756, 0.7065401330633077, 0.6210149130506176, 0.31151340996695076, 0.17900396297140486, 0.3143282857449532, 0.9976777169871904, 0.9228463287667512, 0.03046186733964229, 0.5437444771019777, 0.8383612876120285, 0.4652335921355346, 0.7138384057554705, 0.48859914813179894, 0.09667380777607082, 0.9108293981781888, 0.8536453676307582, 0.023959817006935724, 0.0020187775181281786, 0.4969392533408069, 0.5122946432473341, 0.1998681765754181, 0.9831817179386793, 0.5647086423331322, 0.3323588612300803, 0.8469112555927449, 0.01648535896977954, 0.047187066366171315, 0.5178127256963723, 0.2741018148700387, 0.830186809763789, 0.6875965202136168, 0.09766153663668031, 0.6597960676990929, 0.385792715416158, 0.6028251173382392, 0.04367118678734938, 0.5416144717929076, 0.4860512155804029, 0.8770492217135487, 0.9182481296872457, 0.9141530374114292, 0.015054738408748158, 0.32663107803424274, 0.17597526024255694, 0.23184002405066173, 0.1787138269588574, 0.3625629104231586, 0.549653496324607, 0.964683809085134, 0.7591264934302464, 0.37556206638511436, 0.17903441796348718, 0.1890819121978412, 0.20834923389554338, 0.49090967875191793, 0.1590199167560279, 0.511988800940461, 0.5866773314876601, 0.13758281560562047, 0.27871212064726336, 0.1340758068645692, 0.4198140542609081, 0.6847814105283254, 0.4508361373155464, 0.2738774494935342, 0.3569672438420205, 0.13125213265309021, 0.7622757166228188, 0.3032257300915234, 0.9141928467500948, 0.40480647828879956, 0.9389330441906586, 0.013438060009808628, 0.045005823883364604, 0.8884231364841136, 0.6019894914488145, 0.3140613129900719, 0.9385722023673255, 0.6360505169817873, 0.35920695579412765, 0.9097156579873926, 0.5888281354707438, 0.6416882351272257, 0.14243738912191184, 0.014852516559718087, 0.12061682791997286, 0.08713695771963703, 0.7589916544744181, 0.5658979197115483, 0.7457431417532545, 0.6875353034700966, 0.5325104117505943, 0.12131146102659707, 0.6891771565324186, 0.7725290372494218, 0.8722406632060247, 0.3227156279666783, 0.6919281910229846, 0.38760345781094885, 0.9184322657815907, 0.8056661342409066, 0.04904360732896296, 0.3080038171820786, 0.6235430755243099, 0.0527092810728379, 0.7698588784486421, 0.16768227635827548, 0.9589180352632245, 0.7170585638318213, 0.04846721326212777, 0.9145199165093874, 0.7598610558076571, 0.4330636097199472, 0.05869820015733185, 0.9995243567179203]),\n",
      "    np.array([0.5375978132894877, 0.3786493238028408, 0.37316449464951007, 0.045812943235163384, 0.20272785765419965, 0.9587397718559197, 0.9750501761080087, 0.5525876669199793, 0.7768209035176779, 0.7114956372449387, 0.7909185084839617, 0.3920579709165114, 0.8053439112984044, 0.9419346914424942, 0.311550725523864, 0.4504960735865353, 0.4745812658573859, 0.6117413385935931, 0.5427392840464159, 0.3340147315833918, 0.21421647549477207, 0.042149053256099744, 0.5015569579528681, 0.8109027038692381, 0.61423555829088, 0.15043776153760557, 0.9199655555911271, 0.4689869695024108, 0.43108819814618105, 0.3387377105100574, 0.14005794571310926, 0.5132950589465962, 0.035019627153311306, 0.31733864349039964, 0.5157003441135496, 0.7453198499685687, 0.8240606457694367, 0.43704950054828684, 0.232299020784148, 0.5235574614140245, 0.8776016605442778, 0.29989260157303854, 0.9974043490037167, 0.3537994643816875, 0.494566132415307, 0.11706400095592262, 0.26921352175409785, 0.0017084095519325215, 0.05234094794759048, 0.9759126160456137, 0.21068465385207913, 0.817633356418861, 0.7723129496328999, 0.12486406511939507, 0.17198222017867537, 0.6831620502363506, 0.08328541628429109, 0.39204740052838116, 0.4427600790846702, 0.9436846701436703, 0.7619338789304171, 0.5893254073555507, 0.03002880804485164, 0.23919663037899108, 0.8132494435565681, 0.32768973183605266, 0.8944217124889619, 0.7523496146975837, 0.5411234670534526, 0.07684780198788377, 0.5183576060064321, 0.47153112447305234, 0.4037779712269435, 0.8243326583388383, 0.26964256748969717, 0.624908487685685, 0.0442387493126738, 0.7326005271980366, 0.48767764681407055, 0.5959169955260807, 0.16671200267954878, 0.029575071623908156, 0.40174744529125306, 0.20721709386520182, 0.017042801418744258, 0.3646743139146157, 0.860714188148543, 0.37815034633337197, 0.5649686018664487, 0.4660131958348416, 0.46422365034897417, 0.7421675154028949, 0.5741966954021582, 0.771511723429004, 0.9780798330795007, 0.9457138067388868, 0.01868793648221667, 0.5666265871572655, 0.6192953233012815, 0.636179955252202, 0.7768942490299221, 0.5366744621187971, 0.5471114934430648, 0.8451231611394623, 0.38658458452039857, 0.32374421850788127, 0.3644651834626661, 0.32634139516196237, 0.20350837601003569, 0.2752901279782277, 0.11157758315598376, 0.20175067715979966, 0.24868145490135185, 0.6051785084839134, 0.1646880178434046, 0.6819287870608594, 0.4045700429607979, 0.46758599441850923, 0.3478979492039218, 0.6669500488512203, 0.018408633214923698, 0.07920912902370558, 0.9374357085538143, 0.16192656807657602, 0.5540320432036568, 0.3941077199915647]),\n",
      "    np.array([0.09051211332505282, 0.5155750493984266, 0.7384320693641118, 0.7932403739749015, 0.3292591957150005, 0.817590138369455, 0.7878590671046162, 0.02990028956746671, 0.13636823575283752, 0.9746684572416522, 0.9467785807515033, 0.944238921926306, 0.20674822467511644, 0.871834832066261, 0.9373833049429324, 0.5478734866310614, 0.5589106837959591, 0.308405875438147, 0.33511402559885606, 0.08548172326832093, 0.520818194825275, 0.4622085967779628, 0.4928656244159546, 0.10478963027696375, 0.8639239065444575, 0.032070416189446393, 0.5153424722942266, 0.8815352129800568, 0.6746071445364462, 0.9726932359651482, 0.9167992035422378, 0.1737257728187217, 0.24299928936474757, 0.9590651821992199, 0.9852433037079108, 0.11409530868861362, 0.9623062755784546, 0.16749592748806041, 0.09783536547994032, 0.6387306127818514, 0.014936434179522284, 0.5742602316389729, 0.19420818781691584, 0.5226836048871722, 0.5706027360246132, 0.4515033515574325, 0.6181781339856435, 0.6490819285174526, 0.517352302167618, 0.4178418319082775, 0.7242362544207823, 0.7986084876993892, 0.5537349878275662, 0.44164616906121346, 0.8434218455112457, 0.31714855667474895, 0.705999909088497, 0.8842311740548745, 0.14011361318862992, 0.9552624573702605, 0.6817166713536549, 0.15085747119151727, 0.9700524251349866, 0.7937607687672289, 0.2827387804194832, 0.6708083104237332, 0.7121746792379136, 0.8685066944007049, 0.5059715294977423, 0.46926916457369494, 0.8283044004076966, 0.12523412242208687, 0.8342531205378155, 0.802059028128625, 0.1346060392959515, 0.9745778222276595, 0.15627787360406376, 0.5257968204395282, 0.17407694508631488, 0.2525366423419336, 0.31730374362818603, 0.6819485908742142, 0.5159557881771545, 0.3779669449040569, 0.8945144147859297, 0.862551378018376, 0.8911568465840314, 0.6098728124961839, 0.4998426006142098, 0.24270357502733708, 0.9923036744127035, 0.09438551303114828, 0.6809831091738215, 0.19027326600311323, 0.9909441884132, 0.9505653418199905, 0.206176798493932, 0.05818970779339172, 0.5036689617447517, 0.8731936371242704, 0.011890351375958397, 0.2152665420003257, 0.8118672806510401, 0.4517257961474317, 0.9385847159156422, 0.6881455343623278, 0.5062977738700264, 0.7067534912685932, 0.18807166631767858, 0.8287904133326787, 0.27785875633085355, 0.7356317349632803, 0.17312288525318031, 0.5130342281189404, 0.4243810977740823, 0.6279177548167896, 0.9442260114752871, 0.03605020286674465, 0.3479641855401303, 0.5861045325495494, 0.43716698056884373, 0.10687797804866817, 0.47760132503040587, 0.8428690028192034, 0.0722410083478201, 0.24201814299941105])\n",
      "]\n",
      "\n",
      "# Define the corresponding labels\n",
      "labels = ['sitting', 'walking', 'standing']\n",
      "\n",
      "# Define the new data\n",
      "new_data = np.array([0.014226506617652901, 0.9721416795158672, 0.887687212504904, 0.4703484470760968, 0.6627220787015569, 0.9597487354878672, 0.34368022702930856, 0.7720991997154879, 0.2453610456843619, 0.48047901699968387, 0.0025840134184398345, 0.5710739757751114, 0.8731477128534112, 0.5227592044854527, 0.9023035916565754, 0.2179717275889066, 0.6621055681980209, 0.02521496408306656, 0.6532904651753444, 0.5080074792444526])\n",
      "\n",
      "# Calculate the distances between the new data and the training data\n",
      "distances = [np.linalg.norm(new_data - train_data[i]) for i in range(len(train_data))]\n",
      "\n",
      "# Find the index of the minimum distance\n",
      "min_distance_index = np.argmin(distances)\n",
      "\n",
      "# Print the predicted label\n",
      "print('Predicted label:', labels[min_distance_index])\n",
      "```\n",
      "\n",
      "This code calculates the Euclidean distance between the new data and each of the training data points, and then finds the index of the minimum distance. The predicted label is then the label corresponding to the training data point with the minimum distance.\n",
      "\n",
      "When you run this code, it will print the predicted label for the new data.\n",
      "Actual Activity: sitting\n",
      "\n",
      "Sample 2 Comparison:\n",
      "Zero-Shot Predicted Activity: Zero-Shot Learning for Human Activity Classification\n",
      "=====================================================\n",
      "\n",
      "In this example, we'll use a zero-shot learning approach to classify human activities based on featurized accelerometer data. We'll use the `transformers` library from Hugging Face to leverage pre-trained models and fine-tune them for our specific task.\n",
      "\n",
      "**Required Libraries**\n",
      "\n",
      "* `transformers`\n",
      "* `torch`\n",
      "* `numpy`\n",
      "* `pandas`\n",
      "\n",
      "**Data Preparation**\n",
      "\n",
      "First, let's prepare our data. We'll assume that we have a dataset with featurized accelerometer data and corresponding labels.\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Load the dataset\n",
      "data = pd.read_csv('data.csv')\n",
      "\n",
      "# Split the data into features and labels\n",
      "features = data.drop('label', axis=1)\n",
      "labels = data['label']\n",
      "```\n",
      "\n",
      "**Data Preprocessing**\n",
      "\n",
      "Next, we'll preprocess our data by normalizing the features and converting the labels to a numerical format.\n",
      "\n",
      "```python\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Normalize the features\n",
      "scaler = StandardScaler()\n",
      "features = scaler.fit_transform(features)\n",
      "\n",
      "# Convert the labels to a numerical format\n",
      "labels = pd.get_dummies(labels).values\n",
      "```\n",
      "\n",
      "**Model Selection**\n",
      "\n",
      "For this task, we'll use a pre-trained `BERT` model from the `transformers` library. We'll fine-tune the model on our dataset to adapt it to our specific task.\n",
      "\n",
      "```python\n",
      "from transformers import BertTokenizer, BertModel\n",
      "\n",
      "# Load the pre-trained BERT model and tokenizer\n",
      "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      "model = BertModel.from_pretrained('bert-base-uncased')\n",
      "```\n",
      "\n",
      "**Model Fine-Tuning**\n",
      "\n",
      "Next, we'll fine-tune the pre-trained model on our dataset. We'll use a custom dataset class to handle our data and a `Trainer` object to manage the training process.\n",
      "\n",
      "```python\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "from transformers import Trainer, TrainingArguments\n",
      "\n",
      "# Define a custom dataset class\n",
      "class ActivityDataset(Dataset):\n",
      "    def __init__(self, features, labels):\n",
      "        self.features = features\n",
      "        self.labels = labels\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.features)\n",
      "\n",
      "    def __getitem__(self, idx):\n",
      "        feature = self.features[idx]\n",
      "        label = self.labels[idx]\n",
      "\n",
      "        # Convert the feature to a tensor\n",
      "        feature = torch.tensor(feature, dtype=torch.float32)\n",
      "\n",
      "        # Convert the label to a tensor\n",
      "        label = torch.tensor(label, dtype=torch.long)\n",
      "\n",
      "        return {'input_ids': feature, 'labels': label}\n",
      "\n",
      "# Create a dataset object\n",
      "dataset = ActivityDataset(features, labels)\n",
      "\n",
      "# Create a data loader object\n",
      "batch_size = 32\n",
      "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
      "\n",
      "# Define the training arguments\n",
      "training_args = TrainingArguments(\n",
      "    output_dir='./results',\n",
      "    num_train_epochs=3,\n",
      "    per_device_train_batch_size=batch_size,\n",
      "    per_device_eval_batch_size=batch_size,\n",
      "    evaluation_strategy='epoch',\n",
      "    learning_rate=1e-5,\n",
      "    save_total_limit=2,\n",
      "    save_steps=500,\n",
      "    load_best_model_at_end=True,\n",
      "    metric_for_best_model='accuracy',\n",
      "    greater_is_better=True,\n",
      "    save_on_each_node=True,\n",
      ")\n",
      "\n",
      "# Create a trainer object\n",
      "trainer = Trainer(\n",
      "    model=model,\n",
      "    args=training_args,\n",
      "    train_dataset=dataset,\n",
      "    eval_dataset=dataset,\n",
      ")\n",
      "\n",
      "# Train the model\n",
      "trainer.train()\n",
      "```\n",
      "\n",
      "**Model Evaluation**\n",
      "\n",
      "After training the model, we can evaluate its performance on a test dataset.\n",
      "\n",
      "```python\n",
      "# Evaluate the model\n",
      "trainer.evaluate()\n",
      "```\n",
      "\n",
      "**Zero-Shot Learning**\n",
      "\n",
      "To perform zero-shot learning, we can use the trained model to classify new, unseen data without any additional training. We'll use the `predict` method of the `Trainer` object to generate predictions for new data.\n",
      "\n",
      "```python\n",
      "# Define new, unseen data\n",
      "new_data = np.array([...])  # Replace with new data\n",
      "\n",
      "# Convert the new data to a tensor\n",
      "new_data = torch.tensor(new_data, dtype=torch.float32)\n",
      "\n",
      "# Generate predictions for the new data\n",
      "predictions = trainer.predict(new_data)\n",
      "\n",
      "# Print the predictions\n",
      "print(predictions)\n",
      "```\n",
      "\n",
      "This code provides a basic example of how to use zero-shot learning for human activity classification using featurized accelerometer data. You can modify the code to suit your specific needs and experiment with different models and hyperparameters to improve performance.\n",
      "Few-Shot Predicted Activity: To classify the new data, we can use a few-shot learning approach. Since we have only three examples of different activities (sitting, walking, and standing), we can use a simple k-nearest neighbors (KNN) algorithm to classify the new data.\n",
      "\n",
      "Here's a Python implementation of the KNN algorithm:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from scipy.spatial import distance\n",
      "\n",
      "# Define the training data\n",
      "sitting_data = np.array([0.43285953675395783, 0.2743450980543033, 0.6870501079486016, 0.3998785751741558, 0.22948025718768883, 0.8529979549798196, 0.8782577095331902, 0.3539210421900467, 0.3198987259222472, 0.02917777646929809, 0.47068594145052756, 0.7065401330633077, 0.6210149130506176, 0.31151340996695076, 0.17900396297140486, 0.3143282857449532, 0.9976777169871904, 0.9228463287667512, 0.03046186733964229, 0.5437444771019777, 0.8383612876120285, 0.4652335921355346, 0.7138384057554705, 0.48859914813179894, 0.09667380777607082, 0.9108293981781888, 0.8536453676307582, 0.023959817006935724, 0.0020187775181281786, 0.4969392533408069, 0.5122946432473341, 0.1998681765754181, 0.9831817179386793, 0.5647086423331322, 0.3323588612300803, 0.8469112555927449, 0.01648535896977954, 0.047187066366171315, 0.5178127256963723, 0.2741018148700387, 0.830186809763789, 0.6875965202136168, 0.09766153663668031, 0.6597960676990929, 0.385792715416158, 0.6028251173382392, 0.04367118678734938, 0.5416144717929076, 0.4860512155804029, 0.8770492217135487, 0.9182481296872457, 0.9141530374114292, 0.015054738408748158, 0.32663107803424274, 0.17597526024255694, 0.23184002405066173, 0.1787138269588574, 0.3625629104231586, 0.549653496324607, 0.964683809085134, 0.7591264934302464, 0.37556206638511436, 0.17903441796348718, 0.1890819121978412, 0.20834923389554338, 0.49090967875191793, 0.1590199167560279, 0.511988800940461, 0.5866773314876601, 0.13758281560562047, 0.27871212064726336, 0.1340758068645692, 0.4198140542609081, 0.6847814105283254, 0.4508361373155464, 0.2738774494935342, 0.3569672438420205, 0.13125213265309021, 0.7622757166228188, 0.3032257300915234, 0.9141928467500948, 0.40480647828879956, 0.9389330441906586, 0.013438060009808628, 0.045005823883364604, 0.8884231364841136, 0.6019894914488145, 0.3140613129900719, 0.9385722023673255, 0.6360505169817873, 0.35920695579412765, 0.9097156579873926, 0.5888281354707438, 0.6416882351272257, 0.14243738912191184, 0.014852516559718087, 0.12061682791997286, 0.08713695771963703, 0.7589916544744181, 0.5658979197115483, 0.7457431417532545, 0.6875353034700966, 0.5325104117505943, 0.12131146102659707, 0.6891771565324186, 0.7725290372494218, 0.8722406632060247, 0.3227156279666783, 0.6919281910229846, 0.38760345781094885, 0.9184322657815907, 0.8056661342409066, 0.04904360732896296, 0.3080038171820786, 0.6235430755243099, 0.0527092810728379, 0.7698588784486421, 0.16768227635827548, 0.9589180352632245, 0.7170585638318213, 0.04846721326212777, 0.9145199165093874, 0.7598610558076571, 0.4330636097199472, 0.05869820015733185, 0.9995243567179203])\n",
      "walking_data = np.array([0.5375978132894877, 0.3786493238028408, 0.37316449464951007, 0.045812943235163384, 0.20272785765419965, 0.9587397718559197, 0.9750501761080087, 0.5525876669199793, 0.7768209035176779, 0.7114956372449387, 0.7909185084839617, 0.3920579709165114, 0.8053439112984044, 0.9419346914424942, 0.311550725523864, 0.4504960735865353, 0.4745812658573859, 0.6117413385935931, 0.5427392840464159, 0.3340147315833918, 0.21421647549477207, 0.042149053256099744, 0.5015569579528681, 0.8109027038692381, 0.61423555829088, 0.15043776153760557, 0.9199655555911271, 0.4689869695024108, 0.43108819814618105, 0.3387377105100574, 0.14005794571310926, 0.5132950589465962, 0.035019627153311306, 0.31733864349039964, 0.5157003441135496, 0.7453198499685687, 0.8240606457694367, 0.43704950054828684, 0.232299020784148, 0.5235574614140245, 0.8776016605442778, 0.29989260157303854, 0.9974043490037167, 0.3537994643816875, 0.494566132415307, 0.11706400095592262, 0.26921352175409785, 0.0017084095519325215, 0.05234094794759048, 0.9759126160456137, 0.21068465385207913, 0.817633356418861, 0.7723129496328999, 0.12486406511939507, 0.17198222017867537, 0.6831620502363506, 0.08328541628429109, 0.39204740052838116, 0.4427600790846702, 0.9436846701436703, 0.7619338789304171, 0.5893254073555507, 0.03002880804485164, 0.23919663037899108, 0.8132494435565681, 0.32768973183605266, 0.8944217124889619, 0.7523496146975837, 0.5411234670534526, 0.07684780198788377, 0.5183576060064321, 0.47153112447305234, 0.4037779712269435, 0.8243326583388383, 0.26964256748969717, 0.624908487685685, 0.0442387493126738, 0.7326005271980366, 0.48767764681407055, 0.5959169955260807, 0.16671200267954878, 0.029575071623908156, 0.40174744529125306, 0.20721709386520182, 0.017042801418744258, 0.3646743139146157, 0.860714188148543, 0.37815034633337197, 0.5649686018664487, 0.4660131958348416, 0.46422365034897417, 0.7421675154028949, 0.5741966954021582, 0.771511723429004, 0.9780798330795007, 0.9457138067388868, 0.01868793648221667, 0.5666265871572655, 0.6192953233012815, 0.636179955252202, 0.7768942490299221, 0.5366744621187971, 0.5471114934430648, 0.8451231611394623, 0.38658458452039857, 0.32374421850788127, 0.3644651834626661, 0.32634139516196237, 0.20350837601003569, 0.2752901279782277, 0.11157758315598376, 0.20175067715979966, 0.24868145490135185, 0.6051785084839134, 0.1646880178434046, 0.6819287870608594, 0.4045700429607979, 0.46758599441850923, 0.3478979492039218, 0.6669500488512203, 0.018408633214923698, 0.07920912902370558, 0.9374357085538143, 0.16192656807657602, 0.5540320432036568, 0.3941077199915647])\n",
      "standing_data = np.array([0.09051211332505282, 0.5155750493984266, 0.7384320693641118, 0.7932403739749015, 0.3292591957150005, 0.817590138369455, 0.7878590671046162, 0.02990028956746671, 0.13636823575283752, 0.9746684572416522, 0.9467785807515033, 0.944238921926306, 0.20674822467511644, 0.871834832066261, 0.9373833049429324, 0.5478734866310614, 0.5589106837959591, 0.308405875438147, 0.33511402559885606, 0.08548172326832093, 0.520818194825275, 0.4622085967779628, 0.4928656244159546, 0.10478963027696375, 0.8639239065444575, 0.032070416189446393, 0.5153424722942266, 0.8815352129800568, 0.6746071445364462, 0.9726932359651482, 0.9167992035422378, 0.1737257728187217, 0.24299928936474757, 0.9590651821992199, 0.9852433037079108, 0.11409530868861362, 0.9623062755784546, 0.16749592748806041, 0.09783536547994032, 0.6387306127818514, 0.014936434179522284, 0.5742602316389729, 0.19420818781691584, 0.5226836048871722, 0.5706027360246132, 0.4515033515574325, 0.6181781339856435, 0.6490819285174526, 0.517352302167618, 0.4178418319082775, 0.7242362544207823, 0.7986084876993892, 0.5537349878275662, 0.44164616906121346, 0.8434218455112457, 0.31714855667474895, 0.705999909088497, 0.8842311740548745, 0.14011361318862992, 0.9552624573702605, 0.6817166713536549, 0.15085747119151727, 0.9700524251349866, 0.7937607687672289, 0.2827387804194832, 0.6708083104237332, 0.7121746792379136, 0.8685066944007049, 0.5059715294977423, 0.46926916457369494, 0.8283044004076966, 0.12523412242208687, 0.8342531205378155, 0.802059028128625, 0.1346060392959515, 0.9745778222276595, 0.15627787360406376, 0.5257968204395282, 0.17407694508631488, 0.2525366423419336, 0.31730374362818603, 0.6819485908742142, 0.5159557881771545, 0.3779669449040569, 0.8945144147859297, 0.862551378018376, 0.8911568465840314, 0.6098728124961839, 0.4998426006142098, 0.24270357502733708, 0.9923036744127035, 0.09438551303114828, 0.6809831091738215, 0.19027326600311323, 0.9909441884132, 0.9505653418199905, 0.206176798493932, 0.05818970779339172, 0.5036689617447517, 0.8731936371242704, 0.011890351375958397, 0.2152665420003257, 0.8118672806510401, 0.4517257961474317, 0.9385847159156422, 0.6881455343623278, 0.5062977738700264, 0.7067534912685932, 0.18807166631767858, 0.8287904133326787, 0.27785875633085355, 0.7356317349632803, 0.17312288525318031, 0.5130342281189404, 0.4243810977740823, 0.6279177548167896, 0.9442260114752871, 0.03605020286674465, 0.3479641855401303, 0.5861045325495494, 0.43716698056884373, 0.10687797804866817, 0.47760132503040587, 0.8428690028192034, 0.0722410083478201, 0.24201814299941105])\n",
      "new_data = np.array([0.9323095420369216, 0.7566434740335192, 0.7628315073201689, 0.5269739639178449, 0.6852515377538572, 0.8593497956092622, 0.11042382368373105, 0.05996566081506349, 0.03395838553597985, 0.7383779488479058, 0.8310090947582648, 0.6769118092223552, 0.9739991058199662, 0.5573432331259643, 0.10019110482053262, 0.3175899106168797, 0.9695855160773861, 0.0054962930138344746, 0.772860924975499, 0.13272813085190027])\n",
      "\n",
      "# Define the activities\n",
      "activities = ['sitting', 'walking', 'standing']\n",
      "\n",
      "# Calculate the distances between the new data and the training data\n",
      "distances = [distance.euclidean(new_data, sitting_data), distance.euclidean(new_data, walking_data), distance.euclidean(new_data, standing_data)]\n",
      "\n",
      "# Get the index of the minimum distance\n",
      "min_distance_index = np.argmin(distances)\n",
      "\n",
      "# Print the predicted activity\n",
      "print('Predicted activity:', activities[min_distance_index])\n",
      "```\n",
      "\n",
      "When you run this code, it will print the predicted activity based on the minimum distance between the new data and the training data.\n",
      "\n",
      "**Output:**\n",
      "```\n",
      "Predicted activity: standing\n",
      "```\n",
      "\n",
      "This code uses the Euclidean distance to calculate the distance between the new data and the training data. The activity with the minimum distance is considered the predicted activity.\n",
      "Actual Activity: standing\n",
      "\n",
      "Sample 3 Comparison:\n",
      "Zero-Shot Predicted Activity: Zero-Shot Learning for Human Activity Classification\n",
      "=====================================================\n",
      "\n",
      "Zero-shot learning is a type of machine learning where a model is trained to recognize objects or activities that it has not seen before. In this case, we will use zero-shot learning to classify human activities based on featurized accelerometer data.\n",
      "\n",
      "Dataset\n",
      "--------\n",
      "\n",
      "The dataset consists of featurized accelerometer data, which is a sequence of numbers representing the acceleration of the device in three dimensions (x, y, z). The dataset is labeled with the corresponding human activity (e.g., walking, running, sitting, etc.).\n",
      "\n",
      "Model\n",
      "------\n",
      "\n",
      "We will use a transformer-based model, specifically the BERT (Bidirectional Encoder Representations from Transformers) model, which is pre-trained on a large corpus of text data. We will fine-tune the model on our dataset to adapt it to the accelerometer data.\n",
      "\n",
      "Code\n",
      "-----\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "from transformers import BertTokenizer, BertModel\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "# Load the dataset\n",
      "train_data = ...  # load the training data\n",
      "test_data = ...  # load the testing data\n",
      "\n",
      "# Define the model\n",
      "class ActivityClassifier(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(ActivityClassifier, self).__init__()\n",
      "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
      "        self.dropout = nn.Dropout(0.1)\n",
      "        self.classifier = nn.Linear(self.bert.config.hidden_size, 8)  # 8 classes\n",
      "\n",
      "    def forward(self, input_ids, attention_mask):\n",
      "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
      "        pooled_output = outputs.pooler_output\n",
      "        pooled_output = self.dropout(pooled_output)\n",
      "        outputs = self.classifier(pooled_output)\n",
      "        return outputs\n",
      "\n",
      "# Initialize the model, tokenizer, and optimizer\n",
      "model = ActivityClassifier()\n",
      "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
      "\n",
      "# Train the model\n",
      "for epoch in range(5):\n",
      "    model.train()\n",
      "    total_loss = 0\n",
      "    for batch in train_data:\n",
      "        input_ids = batch['input_ids']\n",
      "        attention_mask = batch['attention_mask']\n",
      "        labels = batch['labels']\n",
      "\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        outputs = model(input_ids, attention_mask)\n",
      "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
      "\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        total_loss += loss.item()\n",
      "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_data)}')\n",
      "\n",
      "    model.eval()\n",
      "    with torch.no_grad():\n",
      "        total_correct = 0\n",
      "        for batch in test_data:\n",
      "            input_ids = batch['input_ids']\n",
      "            attention_mask = batch['attention_mask']\n",
      "            labels = batch['labels']\n",
      "\n",
      "            outputs = model(input_ids, attention_mask)\n",
      "            _, predicted = torch.max(outputs, dim=1)\n",
      "            total_correct += (predicted == labels).sum().item()\n",
      "\n",
      "        accuracy = total_correct / len(test_data)\n",
      "        print(f'Test Accuracy: {accuracy:.4f}')\n",
      "```\n",
      "\n",
      "Note that this is just a basic example, and you may need to modify the code to fit your specific use case. Additionally, you will need to preprocess the accelerometer data to convert it into a format that can be fed into the BERT model.\n",
      "\n",
      "Preprocessing the Data\n",
      "-------------------------\n",
      "\n",
      "To preprocess the accelerometer data, you can use the following steps:\n",
      "\n",
      "1.  **Windowing**: Divide the accelerometer data into fixed-size windows (e.g., 10 seconds).\n",
      "2.  **Feature Extraction**: Extract features from each window, such as the mean, standard deviation, and spectral power density.\n",
      "3.  **Normalization**: Normalize the features to have zero mean and unit variance.\n",
      "4.  **Tokenization**: Convert the normalized features into a sequence of tokens that can be fed into the BERT model.\n",
      "\n",
      "Here is an example of how you can preprocess the accelerometer data:\n",
      "```python\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Load the accelerometer data\n",
      "data = ...  # load the accelerometer data\n",
      "\n",
      "# Windowing\n",
      "window_size = 10  # seconds\n",
      "window_step = 5  # seconds\n",
      "windows = []\n",
      "for i in range(0, len(data), window_step):\n",
      "    window = data[i:i+window_size]\n",
      "    windows.append(window)\n",
      "\n",
      "# Feature Extraction\n",
      "features = []\n",
      "for window in windows:\n",
      "    mean = np.mean(window)\n",
      "    std = np.std(window)\n",
      "    spectral_power_density = np.abs(np.fft.fft(window))**2\n",
      "    features.append([mean, std, spectral_power_density])\n",
      "\n",
      "# Normalization\n",
      "scaler = StandardScaler()\n",
      "features = scaler.fit_transform(features)\n",
      "\n",
      "# Tokenization\n",
      "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      "tokens = []\n",
      "for feature in features:\n",
      "    token = tokenizer.encode(feature, return_tensors='pt')\n",
      "    tokens.append(token)\n",
      "\n",
      "# Convert the tokens to a PyTorch dataset\n",
      "dataset = []\n",
      "for token in tokens:\n",
      "    input_ids = token['input_ids'].flatten()\n",
      "    attention_mask = token['attention_mask'].flatten()\n",
      "    dataset.append({'input_ids': input_ids, 'attention_mask': attention_mask})\n",
      "```\n",
      "Note that this is just one example of how you can preprocess the accelerometer data, and you may need to modify the code to fit your specific use case.\n",
      "Few-Shot Predicted Activity: To classify the new data, we can use a few-shot learning approach. Since we have only three examples of each activity, we can use a simple k-nearest neighbors (KNN) algorithm to classify the new data.\n",
      "\n",
      "Here's a Python implementation of the KNN algorithm:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from scipy.spatial import distance\n",
      "\n",
      "# Define the training data\n",
      "sitting_data = np.array([\n",
      "    [0.43285953675395783, 0.2743450980543033, 0.6870501079486016, 0.3998785751741558, 0.22948025718768883, 0.8529979549798196, 0.8782577095331902, 0.3539210421900467, 0.3198987259222472, 0.02917777646929809, 0.47068594145052756, 0.7065401330633077, 0.6210149130506176, 0.31151340996695076, 0.17900396297140486, 0.3143282857449532, 0.9976777169871904, 0.9228463287667512, 0.03046186733964229, 0.5437444771019777, 0.8383612876120285, 0.4652335921355346, 0.7138384057554705, 0.48859914813179894, 0.09667380777607082, 0.9108293981781888, 0.8536453676307582, 0.023959817006935724, 0.0020187775181281786, 0.4969392533408069, 0.5122946432473341, 0.1998681765754181, 0.9831817179386793, 0.5647086423331322, 0.3323588612300803, 0.8469112555927449, 0.01648535896977954, 0.047187066366171315, 0.5178127256963723, 0.2741018148700387, 0.830186809763789, 0.6875965202136168, 0.09766153663668031, 0.6597960676990929, 0.385792715416158, 0.6028251173382392, 0.04367118678734938, 0.5416144717929076, 0.4860512155804029, 0.8770492217135487, 0.9182481296872457, 0.9141530374114292, 0.015054738408748158, 0.32663107803424274, 0.17597526024255694, 0.23184002405066173, 0.1787138269588574, 0.3625629104231586, 0.549653496324607, 0.964683809085134, 0.7591264934302464, 0.37556206638511436, 0.17903441796348718, 0.1890819121978412, 0.20834923389554338, 0.49090967875191793, 0.1590199167560279, 0.511988800940461, 0.5866773314876601, 0.13758281560562047, 0.27871212064726336, 0.1340758068645692, 0.4198140542609081, 0.6847814105283254, 0.4508361373155464, 0.2738774494935342, 0.3569672438420205, 0.13125213265309021, 0.7622757166228188, 0.3032257300915234, 0.9141928467500948, 0.40480647828879956, 0.9389330441906586, 0.013438060009808628, 0.045005823883364604, 0.8884231364841136, 0.6019894914488145, 0.3140613129900719, 0.9385722023673255, 0.6360505169817873, 0.35920695579412765, 0.9097156579873926, 0.5888281354707438, 0.6416882351272257, 0.14243738912191184, 0.014852516559718087, 0.12061682791997286, 0.08713695771963703, 0.7589916544744181, 0.5658979197115483, 0.7457431417532545, 0.6875353034700966, 0.5325104117505943, 0.12131146102659707, 0.6891771565324186, 0.7725290372494218, 0.8722406632060247, 0.3227156279666783, 0.6919281910229846, 0.38760345781094885, 0.9184322657815907, 0.8056661342409066, 0.04904360732896296, 0.3080038171820786, 0.6235430755243099, 0.0527092810728379, 0.7698588784486421, 0.16768227635827548, 0.9589180352632245, 0.7170585638318213, 0.04846721326212777, 0.9145199165093874, 0.7598610558076571, 0.4330636097199472, 0.05869820015733185, 0.9995243567179203]\n",
      "])\n",
      "\n",
      "walking_data = np.array([\n",
      "    [0.5375978132894877, 0.3786493238028408, 0.37316449464951007, 0.045812943235163384, 0.20272785765419965, 0.9587397718559197, 0.9750501761080087, 0.5525876669199793, 0.7768209035176779, 0.7114956372449387, 0.7909185084839617, 0.3920579709165114, 0.8053439112984044, 0.9419346914424942, 0.311550725523864, 0.4504960735865353, 0.4745812658573859, 0.6117413385935931, 0.5427392840464159, 0.3340147315833918, 0.21421647549477207, 0.042149053256099744, 0.5015569579528681, 0.8109027038692381, 0.61423555829088, 0.15043776153760557, 0.9199655555911271, 0.4689869695024108, 0.43108819814618105, 0.3387377105100574, 0.14005794571310926, 0.5132950589465962, 0.035019627153311306, 0.31733864349039964, 0.5157003441135496, 0.7453198499685687, 0.8240606457694367, 0.43704950054828684, 0.232299020784148, 0.5235574614140245, 0.8776016605442778, 0.29989260157303854, 0.9974043490037167, 0.3537994643816875, 0.494566132415307, 0.11706400095592262, 0.26921352175409785, 0.0017084095519325215, 0.05234094794759048, 0.9759126160456137, 0.21068465385207913, 0.817633356418861, 0.7723129496328999, 0.12486406511939507, 0.17198222017867537, 0.6831620502363506, 0.08328541628429109, 0.39204740052838116, 0.4427600790846702, 0.9436846701436703, 0.7619338789304171, 0.5893254073555507, 0.03002880804485164, 0.23919663037899108, 0.8132494435565681, 0.32768973183605266, 0.8944217124889619, 0.7523496146975837, 0.5411234670534526, 0.07684780198788377, 0.5183576060064321, 0.47153112447305234, 0.4037779712269435, 0.8243326583388383, 0.26964256748969717, 0.624908487685685, 0.0442387493126738, 0.7326005271980366, 0.48767764681407055, 0.5959169955260807, 0.16671200267954878, 0.029575071623908156, 0.40174744529125306, 0.20721709386520182, 0.017042801418744258, 0.3646743139146157, 0.860714188148543, 0.37815034633337197, 0.5649686018664487, 0.4660131958348416, 0.46422365034897417, 0.7421675154028949, 0.5741966954021582, 0.771511723429004, 0.9780798330795007, 0.9457138067388868, 0.01868793648221667, 0.5666265871572655, 0.6192953233012815, 0.636179955252202, 0.7768942490299221, 0.5366744621187971, 0.5471114934430648, 0.8451231611394623, 0.38658458452039857, 0.32374421850788127, 0.3644651834626661, 0.32634139516196237, 0.20350837601003569, 0.2752901279782277, 0.11157758315598376, 0.20175067715979966, 0.24868145490135185, 0.6051785084839134, 0.1646880178434046, 0.6819287870608594, 0.4045700429607979, 0.46758599441850923, 0.3478979492039218, 0.6669500488512203, 0.018408633214923698, 0.07920912902370558, 0.9374357085538143, 0.16192656807657602, 0.5540320432036568, 0.3941077199915647]\n",
      "])\n",
      "\n",
      "standing_data = np.array([\n",
      "    [0.09051211332505282, 0.5155750493984266, 0.7384320693641118, 0.7932403739749015, 0.3292591957150005, 0.817590138369455, 0.7878590671046162, 0.02990028956746671, 0.13636823575283752, 0.9746684572416522, 0.9467785807515033, 0.944238921926306, 0.20674822467511644, 0.871834832066261, 0.9373833049429324, 0.5478734866310614, 0.5589106837959591, 0.308405875438147, 0.33511402559885606, 0.08548172326832093, 0.520818194825275, 0.4622085967779628, 0.4928656244159546, 0.10478963027696375, 0.8639239065444575, 0.032070416189446393, 0.5153424722942266, 0.8815352129800568, 0.6746071445364462, 0.9726932359651482, 0.9167992035422378, 0.1737257728187217, 0.24299928936474757, 0.9590651821992199, 0.9852433037079108, 0.11409530868861362, 0.9623062755784546, 0.16749592748806041, 0.09783536547994032, 0.6387306127818514, 0.014936434179522284, 0.5742602316389729, 0.19420818781691584, 0.5226836048871722, 0.5706027360246132, 0.4515033515574325, 0.6181781339856435, 0.6490819285174526, 0.517352302167618, 0.4178418319082775, 0.7242362544207823, 0.7986084876993892, 0.5537349878275662, 0.44164616906121346, 0.8434218455112457, 0.31714855667474895, 0.705999909088497, 0.8842311740548745, 0.14011361318862992, 0.9552624573702605, 0.6817166713536549, 0.15085747119151727, 0.9700524251349866, 0.7937607687672289, 0.2827387804194832, 0.6708083104237332, 0.7121746792379136, 0.8685066944007049, 0.5059715294977423, 0.46926916457369494, 0.8283044004076966, 0.12523412242208687, 0.8342531205378155, 0.802059028128625, 0.1346060392959515, 0.9745778222276595, 0.15627787360406376, 0.5257968204395282, 0.17407694508631488, 0.2525366423419336, 0.31730374362818603, 0.6819485908742142, 0.5159557881771545, 0.3779669449040569, 0.8945144147859297, 0.862551378018376, 0.8911568465840314, 0.6098728124961839, 0.4998426006142098, 0.24270357502733708, 0.9923036744127035, 0.09438551303114828, 0.6809831091738215, 0.19027326600311323, 0.9909441884132, 0.9505653418199905, 0.206176798493932, 0.05818970779339172, 0.5036689617447517, 0.8731936371242704, 0.011890351375958397, 0.2152665420003257, 0.8118672806510401, 0.4517257961474317, 0.9385847159156422, 0.6881455343623278, 0.5062977738700264, 0.7067534912685932, 0.18807166631767858, 0.8287904133326787, 0.27785875633085355, 0.7356317349632803, 0.17312288525318031, 0.5130342281189404, 0.4243810977740823, 0.6279177548167896, 0.9442260114752871, 0.03605020286674465, 0.3479641855401303, 0.5861045325495494, 0.43716698056884373, 0.10687797804866817, 0.47760132503040587, 0.8428690028192034, 0.0722410083478201, 0.24201814299941105]\n",
      "])\n",
      "\n",
      "# Define the new data\n",
      "new_data = np.array([0.47078067674979485, 0.07797946801306355, 0.07659200225331442, 0.6721228043536813, 0.9317032491340935, 0.6025710157590708, 0.33630607430905946, 0.4389984699707492, 0.43816139341042404, 0.08937384439090346, 0.337026036626198, 0.07248922858483975, 0.6865626074671011, 0.032170774713424444, 0.8594431669595767, 0.739562387971234, 0.5930936596173648, 0.9694251373293877, 0.20702844664468878, 0.08473778756935413])\n",
      "\n",
      "# Calculate the distances between the new data and the training data\n",
      "sitting_distances = [distance.euclidean(new_data, x) for x in sitting_data]\n",
      "walking_distances = [distance.euclidean(new_data, x) for x in walking_data]\n",
      "standing_distances = [distance.euclidean(new_data, x) for x in standing_data]\n",
      "\n",
      "# Calculate the average distance for each activity\n",
      "sitting_average_distance = np.mean(sitting_distances)\n",
      "walking_average_distance = np.mean(walking_distances)\n",
      "standing_average_distance = np.mean(standing_distances)\n",
      "\n",
      "# Determine the activity with the smallest average distance\n",
      "activities = ['sitting', 'walking', 'standing']\n",
      "distances = [sitting_average_distance, walking_average_distance, standing_average_distance]\n",
      "activity = activities[np.argmin(distances)]\n",
      "\n",
      "print(f'The activity is: {activity}')\n",
      "```\n",
      "\n",
      "This code calculates the Euclidean distance between the new data and each of the training data points for each activity. It then calculates the average distance for each activity and determines the activity with the smallest average distance.\n",
      "\n",
      "When you run this code, it will output the activity that is most similar to the new data.\n",
      "\n",
      "Note: This is a simple example and may not work well for more complex datasets. You may need to use more advanced techniques such as machine learning algorithms or deep learning models to achieve better results.\n",
      "Actual Activity: walking\n",
      "\n",
      "Sample 4 Comparison:\n",
      "Zero-Shot Predicted Activity: Zero-Shot Learning for Human Activity Classification\n",
      "=====================================================\n",
      "\n",
      "Zero-shot learning is a type of machine learning where a model is trained to recognize objects or activities that it has not seen before. In this example, we will use a zero-shot learning approach to classify human activities based on featurized accelerometer data.\n",
      "\n",
      "Dataset\n",
      "--------\n",
      "\n",
      "The dataset consists of featurized accelerometer data, which is a sequence of numbers representing the acceleration of a device in three dimensions (x, y, z). The dataset is labeled with the corresponding human activity (e.g., walking, running, sitting, etc.).\n",
      "\n",
      "Model\n",
      "-----\n",
      "\n",
      "We will use a transformer-based model, specifically the BERT (Bidirectional Encoder Representations from Transformers) model, which is pre-trained on a large corpus of text data. We will fine-tune the model on our dataset to adapt it to the task of human activity classification.\n",
      "\n",
      "Code\n",
      "-----\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "from transformers import BertTokenizer, BertModel\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "# Load the dataset\n",
      "train_data = ...  # load the training data\n",
      "test_data = ...  # load the testing data\n",
      "\n",
      "# Define the model\n",
      "class HumanActivityClassifier(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(HumanActivityClassifier, self).__init__()\n",
      "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
      "        self.dropout = nn.Dropout(0.1)\n",
      "        self.classifier = nn.Linear(self.bert.config.hidden_size, 8)  # 8 classes\n",
      "\n",
      "    def forward(self, input_ids, attention_mask):\n",
      "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
      "        pooled_output = outputs.pooler_output\n",
      "        pooled_output = self.dropout(pooled_output)\n",
      "        outputs = self.classifier(pooled_output)\n",
      "        return outputs\n",
      "\n",
      "# Initialize the model, tokenizer, and optimizer\n",
      "model = HumanActivityClassifier()\n",
      "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
      "\n",
      "# Train the model\n",
      "for epoch in range(5):\n",
      "    model.train()\n",
      "    total_loss = 0\n",
      "    for batch in train_data:\n",
      "        input_ids = batch['input_ids']\n",
      "        attention_mask = batch['attention_mask']\n",
      "        labels = batch['labels']\n",
      "\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        outputs = model(input_ids, attention_mask)\n",
      "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
      "\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        total_loss += loss.item()\n",
      "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_data)}')\n",
      "\n",
      "    model.eval()\n",
      "    with torch.no_grad():\n",
      "        total_correct = 0\n",
      "        for batch in test_data:\n",
      "            input_ids = batch['input_ids']\n",
      "            attention_mask = batch['attention_mask']\n",
      "            labels = batch['labels']\n",
      "\n",
      "            outputs = model(input_ids, attention_mask)\n",
      "            _, predicted = torch.max(outputs, dim=1)\n",
      "            total_correct += (predicted == labels).sum().item()\n",
      "\n",
      "        accuracy = total_correct / len(test_data)\n",
      "        print(f'Test Accuracy: {accuracy:.4f}')\n",
      "\n",
      "# Use the model for zero-shot learning\n",
      "def zero_shot_learning(model, input_data):\n",
      "    input_ids = tokenizer.encode(input_data, return_tensors='pt')\n",
      "    attention_mask = tokenizer.encode(input_data, return_tensors='pt', max_length=512, padding='max_length', truncation=True)\n",
      "\n",
      "    outputs = model(input_ids, attention_mask)\n",
      "    _, predicted = torch.max(outputs, dim=1)\n",
      "    return predicted.item()\n",
      "\n",
      "# Test the model on a new, unseen activity\n",
      "new_activity = [0.5240406973224825, 0.5375522579250054, 0.7787984991969, 0.8296867573488613, 0.784915475027815, 0.29092962709032166, 0.3336820624475618, 0.050092186715465226, 0.11055750980443735, 0.9684302022578345, 0.5173981724024954, 0.9579221486609394, 0.6603335310407616, 0.6409665423741713, 0.4811758816236188, 0.9317795303771103, 0.6703123075243471, 0.9272612595760783, 0.9090417000132728, 0.6769635566192063, 0.09745032719220936, 0.8664525310209727, 0.2265357808568914, 0.46057268082639014, 0.484184974005488, 0.18033888913414908, 0.05790082226806159, 0.1929075837785842, 0.5288735203103644, 0.25151424191965643, 0.8736778225945405, 0.4830571964263063, 0.1924538952435304, 0.7108389356338276, 0.639533009047603, 0.04102678593180076, 0.4451769498878687, 0.006467840202812769, 0.9562987706224848, 0.8531985561157178, 0.6965565365648331, 0.3187326640586684, 0.44280727313310786, 0.9087000218596044, 0.343501853720692, 0.05044292134414052, 0.7175170580693523, 0.7338360426474001, 0.08791330645162598, 0.7289836070193361, 0.4452108843452507, 0.4477748470105004, 0.20332984738501492, 0.7011848213922265, 0.5613650271833005, 0.7205345623397111, 0.03090415398251889, 0.319888235657414, 0.06971060025432951, 0.8852285514458111, 0.003434395559295478, 0.8154934166765024, 0.8545588025867618, 0.7795370344332655, 0.3501458742798612, 0.23508911223935447, 0.8256053131153746, 0.25179196598846776, 9.269760278518557e-05, 0.534497500383601, 0.9159118100463778, 0.5711267750925234, 0.01858212660962577, 0.6442033925897006, 0.7304441911318924, 0.9549093242006363, 0.5902667386489712, 0.8732749224627676, 0.7102894631198449, 0.8984240197075835, 0.44705706669359513, 0.29219575869515424, 0.7697448752278393, 0.6272156135323552, 0.19612772598928185, 0.29528249701374054, 0.21775197982783723, 0.497045365099538, 0.9829996321615317, 0.48347243991560174, 0.6687369495623378, 0.9855204099839306, 0.7108033321450896, 0.5635853627709251, 0.6310645367049817, 0.4801068525588785, 0.29972249056502975, 0.15793415637881747, 0.10902977904050326, 0.06169823495233895, 0.1212930034722467, 0.9782129296818698, 0.3884339343635953, 0.041154015265639, 0.6190415059598638, 0.5462063630661085, 0.31030476579662003, 0.463170201491121, 0.7302274297158479, 0.6211983776884508, 0.07223496856260714, 0.017224809922455675, 0.43758549957387316, 0.6104424029203973, 0.9206466811932416, 0.9987065638071149, 0.5528839356116171, 0.8511310222988618, 0.07783674117542427, 0.5748599548349972, 0.801685684131206, 0.73959737291369, 0.6150152506218403, 0.10119421355714153, 0.027169467863802366, 0.46843483617730186]\n",
      "new_activity = torch.tensor(new_activity)\n",
      "\n",
      "predicted_activity = zero_shot_learning(model, new_activity)\n",
      "print(f'Predicted activity: {predicted_activity}')\n",
      "```\n",
      "\n",
      "This code defines a transformer-based model for human activity classification and fine-tunes it on the provided dataset. It then uses the model for zero-shot learning to classify a new, unseen activity.\n",
      "\n",
      "Note that this is just an example code and may require modifications to work with your specific dataset and use case. Additionally, the performance of the model may vary depending on the quality of the dataset and the complexity of the activities being classified.\n",
      "Few-Shot Predicted Activity: To classify the new data, we can use a few-shot learning approach. Since we have only three examples of each activity, we can use a simple k-nearest neighbors (KNN) algorithm to classify the new data.\n",
      "\n",
      "Here's a Python implementation of the KNN algorithm:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "# Define the training data\n",
      "sitting_data = np.array([\n",
      "    [0.43285953675395783, 0.2743450980543033, 0.6870501079486016, 0.3998785751741558, 0.22948025718768883, 0.8529979549798196, 0.8782577095331902, 0.3539210421900467, 0.3198987259222472, 0.02917777646929809, 0.47068594145052756, 0.7065401330633077, 0.6210149130506176, 0.31151340996695076, 0.17900396297140486, 0.3143282857449532, 0.9976777169871904, 0.9228463287667512, 0.03046186733964229, 0.5437444771019777, 0.8383612876120285, 0.4652335921355346, 0.7138384057554705, 0.48859914813179894, 0.09667380777607082, 0.9108293981781888, 0.8536453676307582, 0.023959817006935724, 0.0020187775181281786, 0.4969392533408069, 0.5122946432473341, 0.1998681765754181, 0.9831817179386793, 0.5647086423331322, 0.3323588612300803, 0.8469112555927449, 0.01648535896977954, 0.047187066366171315, 0.5178127256963723, 0.2741018148700387, 0.830186809763789, 0.6875965202136168, 0.09766153663668031, 0.6597960676990929, 0.385792715416158, 0.6028251173382392, 0.04367118678734938, 0.5416144717929076, 0.4860512155804029, 0.8770492217135487, 0.9182481296872457, 0.9141530374114292, 0.015054738408748158, 0.32663107803424274, 0.17597526024255694, 0.23184002405066173, 0.1787138269588574, 0.3625629104231586, 0.549653496324607, 0.964683809085134, 0.7591264934302464, 0.37556206638511436, 0.17903441796348718, 0.1890819121978412, 0.20834923389554338, 0.49090967875191793, 0.1590199167560279, 0.511988800940461, 0.5866773314876601, 0.13758281560562047, 0.27871212064726336, 0.1340758068645692, 0.4198140542609081, 0.6847814105283254, 0.4508361373155464, 0.2738774494935342, 0.3569672438420205, 0.13125213265309021, 0.7622757166228188, 0.3032257300915234, 0.9141928467500948, 0.40480647828879956, 0.9389330441906586, 0.013438060009808628, 0.045005823883364604, 0.8884231364841136, 0.6019894914488145, 0.3140613129900719, 0.9385722023673255, 0.6360505169817873, 0.35920695579412765, 0.9097156579873926, 0.5888281354707438, 0.6416882351272257, 0.14243738912191184, 0.014852516559718087, 0.12061682791997286, 0.08713695771963703, 0.7589916544744181, 0.5658979197115483, 0.7457431417532545, 0.6875353034700966, 0.5325104117505943, 0.12131146102659707, 0.6891771565324186, 0.7725290372494218, 0.8722406632060247, 0.3227156279666783, 0.6919281910229846, 0.38760345781094885, 0.9184322657815907, 0.8056661342409066, 0.04904360732896296, 0.3080038171820786, 0.6235430755243099, 0.0527092810728379, 0.7698588784486421, 0.16768227635827548, 0.9589180352632245, 0.7170585638318213, 0.04846721326212777, 0.9145199165093874, 0.7598610558076571, 0.4330636097199472, 0.05869820015733185, 0.9995243567179203],\n",
      "    [0.5375978132894877, 0.3786493238028408, 0.37316449464951007, 0.045812943235163384, 0.20272785765419965, 0.9587397718559197, 0.9750501761080087, 0.5525876669199793, 0.7768209035176779, 0.7114956372449387, 0.7909185084839617, 0.3920579709165114, 0.8053439112984044, 0.9419346914424942, 0.311550725523864, 0.4504960735865353, 0.4745812658573859, 0.6117413385935931, 0.5427392840464159, 0.3340147315833918, 0.21421647549477207, 0.042149053256099744, 0.5015569579528681, 0.8109027038692381, 0.61423555829088, 0.15043776153760557, 0.9199655555911271, 0.4689869695024108, 0.43108819814618105, 0.3387377105100574, 0.14005794571310926, 0.5132950589465962, 0.035019627153311306, 0.31733864349039964, 0.5157003441135496, 0.7453198499685687, 0.8240606457694367, 0.43704950054828684, 0.232299020784148, 0.5235574614140245, 0.8776016605442778, 0.29989260157303854, 0.9974043490037167, 0.3537994643816875, 0.494566132415307, 0.11706400095592262, 0.26921352175409785, 0.0017084095519325215, 0.05234094794759048, 0.9759126160456137, 0.21068465385207913, 0.817633356418861, 0.7723129496328999, 0.12486406511939507, 0.17198222017867537, 0.6831620502363506, 0.08328541628429109, 0.39204740052838116, 0.4427600790846702, 0.9436846701436703, 0.7619338789304171, 0.5893254073555507, 0.03002880804485164, 0.23919663037899108, 0.8132494435565681, 0.32768973183605266, 0.8944217124889619, 0.7523496146975837, 0.5411234670534526, 0.07684780198788377, 0.5183576060064321, 0.47153112447305234, 0.4037779712269435, 0.8243326583388383, 0.26964256748969717, 0.624908487685685, 0.0442387493126738, 0.7326005271980366, 0.48767764681407055, 0.5959169955260807, 0.16671200267954878, 0.029575071623908156, 0.40174744529125306, 0.20721709386520182, 0.017042801418744258, 0.3646743139146157, 0.860714188148543, 0.37815034633337197, 0.5649686018664487, 0.4660131958348416, 0.46422365034897417, 0.7421675154028949, 0.5741966954021582, 0.771511723429004, 0.9780798330795007, 0.9457138067388868, 0.01868793648221667, 0.5666265871572655, 0.6192953233012815, 0.636179955252202, 0.7768942490299221, 0.5366744621187971, 0.5471114934430648, 0.8451231611394623, 0.38658458452039857, 0.32374421850788127, 0.3644651834626661, 0.32634139516196237, 0.20350837601003569, 0.2752901279782277, 0.11157758315598376, 0.20175067715979966, 0.24868145490135185, 0.6051785084839134, 0.1646880178434046, 0.6819287870608594, 0.4045700429607979, 0.46758599441850923, 0.3478979492039218, 0.6669500488512203, 0.018408633214923698, 0.07920912902370558, 0.9374357085538143, 0.16192656807657602, 0.5540320432036568, 0.3941077199915647],\n",
      "    [0.09051211332505282, 0.5155750493984266, 0.7384320693641118, 0.7932403739749015, 0.3292591957150005, 0.817590138369455, 0.7878590671046162, 0.02990028956746671, 0.13636823575283752, 0.9746684572416522, 0.9467785807515033, 0.944238921926306, 0.20674822467511644, 0.871834832066261, 0.9373833049429324, 0.5478734866310614, 0.5589106837959591, 0.308405875438147, 0.33511402559885606, 0.08548172326832093, 0.520818194825275, 0.4622085967779628, 0.4928656244159546, 0.10478963027696375, 0.8639239065444575, 0.032070416189446393, 0.5153424722942266, 0.8815352129800568, 0.6746071445364462, 0.9726932359651482, 0.9167992035422378, 0.1737257728187217, 0.24299928936474757, 0.9590651821992199, 0.9852433037079108, 0.11409530868861362, 0.9623062755784546, 0.16749592748806041, 0.09783536547994032, 0.6387306127818514, 0.014936434179522284, 0.5742602316389729, 0.19420818781691584, 0.5226836048871722, 0.5706027360246132, 0.4515033515574325, 0.6181781339856435, 0.6490819285174526, 0.517352302167618, 0.4178418319082775, 0.7242362544207823, 0.7986084876993892, 0.5537349878275662, 0.44164616906121346, 0.8434218455112457, 0.31714855667474895, 0.705999909088497, 0.8842311740548745, 0.14011361318862992, 0.9552624573702605, 0.6817166713536549, 0.15085747119151727, 0.9700524251349866, 0.7937607687672289, 0.2827387804194832, 0.6708083104237332, 0.7121746792379136, 0.8685066944007049, 0.5059715294977423, 0.46926916457369494, 0.8283044004076966, 0.12523412242208687, 0.8342531205378155, 0.802059028128625, 0.1346060392959515, 0.9745778222276595, 0.15627787360406376, 0.5257968204395282, 0.17407694508631488, 0.2525366423419336, 0.31730374362818603, 0.6819485908742142, 0.5159557881771545, 0.3779669449040569, 0.8945144147859297, 0.862551378018376, 0.8911568465840314, 0.6098728124961839, 0.4998426006142098, 0.24270357502733708, 0.9923036744127035, 0.09438551303114828, 0.6809831091738215, 0.19027326600311323, 0.9909441884132, 0.9505653418199905, 0.206176798493932, 0.05818970779339172, 0.5036689617447517, 0.8731936371242704, 0.011890351375958397, 0.2152665420003257, 0.8118672806510401, 0.4517257961474317, 0.9385847159156422, 0.6881455343623278, 0.5062977738700264, 0.7067534912685932, 0.18807166631767858, 0.8287904133326787, 0.27785875633085355, 0.7356317349632803, 0.17312288525318031, 0.5130342281189404, 0.4243810977740823, 0.6279177548167896, 0.9442260114752871, 0.03605020286674465, 0.3479641855401303, 0.5861045325495494, 0.43716698056884373, 0.10687797804866817, 0.47760132503040587, 0.8428690028192034, 0.0722410083478201, 0.24201814299941105]\n",
      "])\n",
      "\n",
      "walking_data = np.array([\n",
      "    [0.5375978132894877, 0.3786493238028408, 0.37316449464951007, 0.045812943235163384, 0.20272785765419965, 0.9587397718559197, 0.9750501761080087, 0.5525876669199793, 0.7768209035176779, 0.7114956372449387, 0.7909185084839617, 0.3920579709165114, 0.8053439112984044, 0.9419346914424942, 0.311550725523864, 0.4504960735865353, 0.4745812658573859, 0.6117413385935931, 0.5427392840464159, 0.3340147315833918, 0.21421647549477207, 0.042149053256099744, 0.5015569579528681, 0.8109027038692381, 0.61423555829088, 0.15043776153760557, 0.9199655555911271, 0.4689869695024108, 0.43108819814618105, 0.3387377105100574, 0.14005794571310926, 0.5132950589465962, 0.035019627153311306, 0.31733864349039964, 0.5157003441135496, 0.7453198499685687, 0.8240606457694367, 0.43704950054828684, 0.232299020784148, 0.5235574614140245, 0.8776016605442778, 0.29989260157303854, 0.9974043490037167, 0.3537994643816875, 0.494566132415307, 0.11706400095592262, 0.26921352175409785, 0.0017084095519325215, 0.05234094794759048, 0.9759126160456137, 0.21068465385207913, 0.817633356418861, 0.7723129496328999, 0.12486406511939507, 0.17198222017867537, 0.6831620502363506, 0.08328541628429109, 0.39204740052838116, 0.4427600790846702, 0.9436846701436703, 0.7619338789304171, 0.5893254073555507, 0.03002880804485164, 0.23919663037899108, 0.8132494435565681, 0.32768973183605266, 0.8944217124889619, 0.7523496146975837, 0.5411234670534526, 0.07684780198788377, 0.5183576060064321, 0.47153112447305234, 0.4037779712269435, 0.8243326583388383, 0.26964256748969717, 0.624908487685685, 0.0442387493126738, 0.7326005271980366, 0.48767764681407055, 0.5959169955260807, 0.16671200267954878, 0.029575071623908156, 0.40174744529125306, 0.20721709386520182, 0.017042801418744258, 0.3646743139146157, 0.860714188148543, 0.37815034633337197, 0.5649686018664487, 0.4660131958348416, 0.46422365034897417, 0.7421675154028949, 0.5741966954021582, 0.771511723429004, 0.9780798330795007, 0.9457138067388868, 0.01868793648221667, 0.5666265871572655, 0.6192953233012815, 0.636179955252202, 0.7768942490299221, 0.5366744621187971, 0.5471114934430648, 0.8451231611394623, 0.38658458452039857, 0.32374421850788127, 0.3644651834626661, 0.32634139516196237, 0.20350837601003569, 0.2752901279782277, 0.11157758315598376, 0.20175067715979966, 0.24868145490135185, 0.6051785084839134, 0.1646880178434046, 0.6819287870608594, 0.4045700429607979, 0.46758599441850923, 0.3478979492039218, 0.6669500488512203, 0.018408633214923698, 0.07920912902370558, 0.9374357085538143, 0.16192656807657602, 0.5540320432036568, 0.3941077199915647],\n",
      "    [0.09051211332505282, 0.5155750493984266, 0.7384320693641118, 0.7932403739749015, 0.3292591957150005, 0.817590138369455, 0.7878590671046162, 0.02990028956746671, 0.13636823575283752, 0.9746684572416522, 0.9467785807515033, 0.944238921926306, 0.20674822467511644, 0.871834832066261, 0.9373833049429324, 0.5478734866310614, 0.5589106837959591, 0.308405875438147, 0.33511402559885606, 0.08548172326832093, 0.520818194825275, 0.4622085967779628, 0.4928656244159546, 0.10478963027696375, 0.8639239065444575, 0.032070416189446393, 0.5153424722942266, 0.8815352129800568, 0.6746071445364462, 0.9726932359651482, 0.9167992035422378, 0.1737257728187217, 0.24299928936474757, 0.9590651821992199, 0.9852433037079108, 0.11409530868861362, 0.9623062755784546, 0.16749592748806041, 0.09783536547994032, 0.6387306127818514, 0.014936434179522284, 0.5742602316389729, 0.19420818781691584, 0.5226836048871722, 0.5706027360246132, 0.4515033515574325, 0.6181781339856435, 0.6490819285174526, 0.517352302167618, 0.4178418319082775, 0.7242362544207823, 0.7986084876993892, 0.5537349878275662, 0.44164616906121346, 0.8434218455112457, 0.31714855667474895, 0.705999909088497, 0.8842311740548745, 0.14011361318862992, 0.9552624573702605, 0.6817166713536549, 0.15085747119151727, 0.9700524251349866, 0.7937607687672289, 0.2827387804194832, 0.6708083104237332, 0.7121746792379136, 0.8685066944007049, 0.5059715294977423, 0.46926916457369494, 0.8283044004076966, 0.12523412242208687, 0.8342531205378155, 0.802059028128625, 0.1346060392959515, 0.9745778222276595, 0.15627787360406376, 0.5257968204395282, 0.17407694508631488, 0.2525366423419336, 0.31730374362818603, 0.6819485908742142, 0.5159557881771545, 0.3779669449040569, 0.8945144147859297, 0.862551378018376, 0.8911568465840314, 0.6098728124961839, 0.4998426006142098, 0.24270357502733708, 0.9923036744127035, 0.09438551303114828, 0.6809831091738215, 0.19027326600311323, 0.9909441884132, 0.9505653418199905, 0.206176798493932, 0.05818970779339172, 0.5036689617447517, 0.8731936371242704, 0.011890351375958397, 0.2152665420003257, 0.8118672806510401, 0.4517257961474317, 0.9385847159156422, 0.6881455343623278, 0.5062977738700264, 0.7067534912685932, 0.18807166631767858, 0.8287904133326787, 0.27785875633085355, 0.7356317349632803, 0.17312288525318031, 0.5130342281189404, 0.4243810977740823, 0.6279177548167896, 0.9442260114752871, 0.03605020286674465, 0.3479641855401303, 0.5861045325495494, 0.43716698056884373, 0.10687797804866817, 0.47760132503040587, 0.8428690028192034, 0.0722410083478201, 0.24201814299941105]\n",
      "])\n",
      "\n",
      "standing_data = np.array([\n",
      "    [0.5240406973224825, 0.5375522579250054, 0.7787984991969, 0.8296867573488613, 0.784915475027815, 0.29092962709032166, 0.3336820624475618, 0.050092186715465226, 0.11055750980443735, 0.9684302022578345, 0.5173981724024954, 0.9579221486609394, 0.6603335310407616, 0.6409665423741713, 0.4811758816236188, 0.9317795303771103, 0.6703123075243471, 0.9272612595760783, 0.9090417000132728, 0.6769635566192063]\n",
      "])\n",
      "\n",
      "# Define the labels for the training data\n",
      "sitting_labels = np.array([0, 0, 0])\n",
      "walking_labels = np.array([1, 1, 1])\n",
      "standing_labels = np.array([2, 2, 2])\n",
      "\n",
      "# Combine the training data and labels\n",
      "X_train = np.concatenate((sitting_data, walking_data, standing_data))\n",
      "y_train = np.concatenate((sitting_labels, walking_labels, standing_labels))\n",
      "\n",
      "# Create a KNN classifier\n",
      "knn = KNeighborsClassifier(n_neighbors=3)\n",
      "\n",
      "# Train the classifier\n",
      "knn.fit(X_train, y_train)\n",
      "\n",
      "# Define the new data to classify\n",
      "new_data = np.array([0.5240406973224825, 0.5375522579250054, 0.7787984991969, 0.8296867573488613, 0.784915475027815, 0.29092962709032166, 0.3336820624475618, 0.050092186715465226, 0.11055750980443735, 0.9684302022578345, 0.5173981724024954, 0.9579221486609394, 0.6603335310407616, 0.6409665423741713, 0.4811758816236188, 0.9317795303771103, 0.6703123075243471, 0.9272612595760783, 0.9090417000132728, 0.6769635566192063])\n",
      "\n",
      "# Classify the new data\n",
      "prediction = knn.predict([new_data])\n",
      "\n",
      "# Print the prediction\n",
      "if prediction[0] == 0:\n",
      "    print(\"The activity is: sitting\")\n",
      "elif prediction[0] == 1:\n",
      "    print(\"The activity is: walking\")\n",
      "else:\n",
      "    print(\"The activity is: standing\")\n",
      "```\n",
      "\n",
      "When you run this code, it will print the predicted activity for the new data.\n",
      "Actual Activity: sitting\n",
      "\n",
      "Sample 5 Comparison:\n",
      "Zero-Shot Predicted Activity: Zero-Shot Learning for Human Activity Classification\n",
      "=====================================================\n",
      "\n",
      "Zero-shot learning is a type of machine learning where a model is trained to recognize objects or activities that it has not seen before. In this case, we will use zero-shot learning to classify human activities based on featurized accelerometer data.\n",
      "\n",
      "Dataset\n",
      "--------\n",
      "\n",
      "The dataset consists of featurized accelerometer data, which is a sequence of numbers representing the acceleration of the device in three dimensions (x, y, z). Each sequence represents a single activity, such as walking, running, or sitting.\n",
      "\n",
      "Model\n",
      "------\n",
      "\n",
      "We will use a transformer-based model, specifically the BERT (Bidirectional Encoder Representations from Transformers) model, which is well-suited for sequence classification tasks.\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "from transformers import BertTokenizer, BertModel\n",
      "\n",
      "class ActivityClassifier(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(ActivityClassifier, self).__init__()\n",
      "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
      "        self.dropout = nn.Dropout(0.1)\n",
      "        self.classifier = nn.Linear(self.bert.config.hidden_size, 8)  # 8 classes\n",
      "\n",
      "    def forward(self, input_ids, attention_mask):\n",
      "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
      "        pooled_output = outputs.pooler_output\n",
      "        pooled_output = self.dropout(pooled_output)\n",
      "        outputs = self.classifier(pooled_output)\n",
      "        return outputs\n",
      "```\n",
      "\n",
      "Training\n",
      "--------\n",
      "\n",
      "We will train the model using a zero-shot learning approach, where we train the model on a set of activities that are not present in the test set.\n",
      "\n",
      "```python\n",
      "# Define the training dataset\n",
      "train_dataset = ActivityDataset(train_data)\n",
      "\n",
      "# Define the data loader\n",
      "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
      "\n",
      "# Define the optimizer and loss function\n",
      "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
      "loss_fn = nn.CrossEntropyLoss()\n",
      "\n",
      "# Train the model\n",
      "for epoch in range(5):\n",
      "    model.train()\n",
      "    total_loss = 0\n",
      "    for batch in train_loader:\n",
      "        input_ids = batch['input_ids'].to(device)\n",
      "        attention_mask = batch['attention_mask'].to(device)\n",
      "        labels = batch['labels'].to(device)\n",
      "\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        outputs = model(input_ids, attention_mask)\n",
      "        loss = loss_fn(outputs, labels)\n",
      "\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        total_loss += loss.item()\n",
      "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}')\n",
      "```\n",
      "\n",
      "Evaluation\n",
      "------------\n",
      "\n",
      "We will evaluate the model on a test set that contains activities that are not present in the training set.\n",
      "\n",
      "```python\n",
      "# Define the test dataset\n",
      "test_dataset = ActivityDataset(test_data)\n",
      "\n",
      "# Define the data loader\n",
      "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
      "\n",
      "# Evaluate the model\n",
      "model.eval()\n",
      "test_loss = 0\n",
      "correct = 0\n",
      "with torch.no_grad():\n",
      "    for batch in test_loader:\n",
      "        input_ids = batch['input_ids'].to(device)\n",
      "        attention_mask = batch['attention_mask'].to(device)\n",
      "        labels = batch['labels'].to(device)\n",
      "\n",
      "        outputs = model(input_ids, attention_mask)\n",
      "        loss = loss_fn(outputs, labels)\n",
      "        test_loss += loss.item()\n",
      "        _, predicted = torch.max(outputs, dim=1)\n",
      "        correct += (predicted == labels).sum().item()\n",
      "\n",
      "accuracy = correct / len(test_dataset)\n",
      "print(f'Test Loss: {test_loss / len(test_loader)}')\n",
      "print(f'Test Accuracy: {accuracy:.4f}')\n",
      "```\n",
      "\n",
      "Example Use Case\n",
      "-----------------\n",
      "\n",
      "Here is an example of how to use the model to classify a new activity:\n",
      "\n",
      "```python\n",
      "# Define the new activity data\n",
      "new_activity = torch.tensor([[0.29366953511084815, 0.4338019848536213, 0.5873021668238246, ...]])\n",
      "\n",
      "# Preprocess the data\n",
      "input_ids = tokenizer.encode(new_activity, return_tensors='pt')\n",
      "attention_mask = tokenizer.encode(new_activity, return_tensors='pt', max_length=512, padding='max_length', truncation=True)\n",
      "\n",
      "# Classify the activity\n",
      "outputs = model(input_ids, attention_mask)\n",
      "_, predicted = torch.max(outputs, dim=1)\n",
      "print(f'Predicted activity: {predicted.item()}')\n",
      "```\n",
      "\n",
      "Note that this is just an example code and you will need to modify it to fit your specific use case. Additionally, you will need to preprocess the data and define the `ActivityDataset` class to load the data.\n",
      "Few-Shot Predicted Activity: To classify the new data, we'll use a few-shot learning approach with a simple neural network. We'll train the model on the provided data and then use it to classify the new data.\n",
      "\n",
      "Here's a Python code snippet using PyTorch to implement this:\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "import numpy as np\n",
      "\n",
      "# Define the dataset class\n",
      "class ActivityDataset(Dataset):\n",
      "    def __init__(self, data, labels):\n",
      "        self.data = torch.tensor(data, dtype=torch.float32)\n",
      "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index], self.labels[index]\n",
      "\n",
      "# Define the model\n",
      "class ActivityClassifier(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(ActivityClassifier, self).__init__()\n",
      "        self.fc1 = nn.Linear(20, 128)  # input layer (20) -> hidden layer (128)\n",
      "        self.fc2 = nn.Linear(128, 3)  # hidden layer (128) -> output layer (3)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = torch.relu(self.fc1(x))  # activation function for hidden layer\n",
      "        x = self.fc2(x)\n",
      "        return x\n",
      "\n",
      "# Define the data and labels\n",
      "data = [\n",
      "    [0.43285953675395783, 0.2743450980543033, 0.6870501079486016, 0.3998785751741558, 0.22948025718768883, 0.8529979549798196, 0.8782577095331902, 0.3539210421900467, 0.3198987259222472, 0.02917777646929809, 0.47068594145052756, 0.7065401330633077, 0.6210149130506176, 0.31151340996695076, 0.17900396297140486, 0.3143282857449532, 0.9976777169871904, 0.9228463287667512, 0.03046186733964229, 0.5437444771019777],\n",
      "    [0.5375978132894877, 0.3786493238028408, 0.37316449464951007, 0.045812943235163384, 0.20272785765419965, 0.9587397718559197, 0.9750501761080087, 0.5525876669199793, 0.7768209035176779, 0.7114956372449387, 0.7909185084839617, 0.3920579709165114, 0.8053439112984044, 0.9419346914424942, 0.311550725523864, 0.4504960735865353, 0.4745812658573859, 0.6117413385935931, 0.5427392840464159, 0.3340147315833918],\n",
      "    [0.09051211332505282, 0.5155750493984266, 0.7384320693641118, 0.7932403739749015, 0.3292591957150005, 0.817590138369455, 0.7878590671046162, 0.02990028956746671, 0.13636823575283752, 0.9746684572416522, 0.9467785807515033, 0.944238921926306, 0.20674822467511644, 0.871834832066261, 0.9373833049429324, 0.5478734866310614, 0.5589106837959591, 0.308405875438147, 0.33511402559885606, 0.08548172326832093]\n",
      "]\n",
      "labels = [0, 1, 2]  # 0: sitting, 1: walking, 2: standing\n",
      "\n",
      "# Create the dataset and data loader\n",
      "dataset = ActivityDataset(data, labels)\n",
      "data_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
      "\n",
      "# Initialize the model, loss function, and optimizer\n",
      "model = ActivityClassifier()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
      "\n",
      "# Train the model\n",
      "for epoch in range(100):\n",
      "    for i, (inputs, labels) in enumerate(data_loader):\n",
      "        # Zero the gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # Forward pass\n",
      "        outputs = model(inputs)\n",
      "\n",
      "        # Calculate the loss\n",
      "        loss = criterion(outputs, labels)\n",
      "\n",
      "        # Backward pass\n",
      "        loss.backward()\n",
      "\n",
      "        # Update the model parameters\n",
      "        optimizer.step()\n",
      "\n",
      "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
      "\n",
      "# Use the trained model to classify the new data\n",
      "new_data = torch.tensor([0.29366953511084815, 0.4338019848536213, 0.5873021668238246, 0.6756516669944583, 0.4223363483985568, 0.27983978620364314, 0.9505191029986243, 0.12555576505588228, 0.31906787414775994, 0.6524929744628959, 0.7398107772872435, 0.16526667586147636, 0.05998084499448564, 0.7347377310773903, 0.8487199664786912, 0.39612830636114293, 0.6932012549960301, 0.3883524441180226, 0.04898711993730964, 0.22217676799095254], dtype=torch.float32)\n",
      "output = model(new_data)\n",
      "_, predicted = torch.max(output, 0)\n",
      "print(f'Predicted activity: {[\"sitting\", \"walking\", \"standing\"][predicted]}')\n",
      "```\n",
      "\n",
      "This code defines a simple neural network with two fully connected layers, trains it on the provided data, and then uses it to classify the new data. The output of the model is the predicted activity.\n",
      "\n",
      "Note that this is a very basic example and the model may not generalize well to new data. You may need to collect more data, preprocess it, and fine-tune the model to achieve better results.\n",
      "Actual Activity: walking\n",
      "\n",
      "Zero-Shot Learning Accuracy: 0.00%\n",
      "Few-Shot Learning Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq.chat_models import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "groq_api_key = os.getenv(\"API_FINAL_KEY\")\n",
    "\n",
    "\n",
    "# Initialize the chat model with specific settings\n",
    "chat = ChatGroq(temperature=0.1, groq_api_key= groq_api_key, model_name=\"llama-3.1-70b-versatile\")\n",
    "\n",
    "# Define the system prompts\n",
    "zero_shot_system = \"Zero-Shot Learning to classify human activities based on the featurized accelerometer data.\"\n",
    "few_shot_system = \"Few-Shot Learning to classify human activities based on the featurized accelerometer data.\"\n",
    "\n",
    "# Define the human prompt template\n",
    "human = \"{text}\"\n",
    "\n",
    "# Create the chains for Zero-Shot and Few-Shot Learning\n",
    "zero_shot_prompt = ChatPromptTemplate.from_messages([(\"system\", zero_shot_system), (\"human\", human)])\n",
    "few_shot_prompt = ChatPromptTemplate.from_messages([(\"system\", few_shot_system), (\"human\", human)])\n",
    "\n",
    "zero_shot_chain = zero_shot_prompt | chat\n",
    "few_shot_chain = few_shot_prompt | chat\n",
    "\n",
    "\n",
    "# Example of synthetic train and test data (replace with actual data)\n",
    "train_data = np.random.rand(100, 126)  # 100 samples, 126 features each\n",
    "train_labels = np.random.choice([\"walking\", \"standing\", \"sitting\"], 100)  # Random labels\n",
    "\n",
    "test_data = np.random.rand(10, 126)  # 10 samples, 126 features each\n",
    "test_labels = np.random.choice([\"walking\", \"standing\", \"sitting\"], 10)  # Corresponding labels\n",
    "\n",
    "# Prepare examples for Few-Shot Learning\n",
    "examples = \"\"\n",
    "for i in range(3):  # Use first 3 samples as examples\n",
    "    data_summary = f\"Data: {train_data[i].tolist()} => Activity: {train_labels[i]}\"\n",
    "    examples += data_summary + \"\\n\"\n",
    "\n",
    "# Generate random test data with the same dimensions and range\n",
    "random_test_data = np.random.uniform(low=np.min(train_data), high=np.max(train_data), size=test_data.shape)\n",
    "\n",
    "# Initialize lists to store predictions\n",
    "zero_shot_predictions = []\n",
    "few_shot_predictions = []\n",
    "\n",
    "# Loop through the test data to make predictions and compare results\n",
    "zero_shot_predictions = []\n",
    "few_shot_predictions = []\n",
    "\n",
    "# Loop through the test data to make predictions and compare results\n",
    "for i in range(5):  \n",
    "    # Zero-Shot Learning\n",
    "    zero_shot_data_summary = f\"Sample data: {test_data[i].flatten().tolist()}...\"  \n",
    "    zero_shot_result = zero_shot_chain.invoke({\"text\": zero_shot_data_summary}).content.strip()\n",
    "    zero_shot_predictions.append(zero_shot_result)\n",
    "\n",
    "    # Few-Shot Learning\n",
    "    new_data_summary = f\"New data: {test_data[i].flatten().tolist()[:20]}\"\n",
    "    few_shot_prompt_text = f\"Here are some examples:\\n{examples}\\nNow classify the following:\\n{new_data_summary}\"\n",
    "    few_shot_result = few_shot_chain.invoke({\"text\": few_shot_prompt_text}).content.strip()\n",
    "    few_shot_predictions.append(few_shot_result)\n",
    "\n",
    "    # Output the comparison for the current sample\n",
    "    print(f\"Sample {i+1} Comparison:\")\n",
    "    print(f\"Zero-Shot Predicted Activity: {zero_shot_result}\")\n",
    "    print(f\"Few-Shot Predicted Activity: {few_shot_result}\")\n",
    "    print(f\"Actual Activity: {test_labels[i]}\\n\")\n",
    "\n",
    "# Calculate accuracy for Zero-Shot Learning\n",
    "zero_shot_accuracy = accuracy_score(test_labels[:5], zero_shot_predictions)\n",
    "print(f\"Zero-Shot Learning Accuracy: {zero_shot_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Calculate accuracy for Few-Shot Learning\n",
    "few_shot_accuracy = accuracy_score(test_labels[:5], few_shot_predictions)\n",
    "print(f\"Few-Shot Learning Accuracy: {few_shot_accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
